{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-15T13:06:56.369168Z",
     "iopub.status.busy": "2025-02-15T13:06:56.368761Z",
     "iopub.status.idle": "2025-02-15T13:06:56.377082Z",
     "shell.execute_reply": "2025-02-15T13:06:56.376185Z",
     "shell.execute_reply.started": "2025-02-15T13:06:56.369142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos cargados: 820834 filas, 68 columnas.\n"
     ]
    }
   ],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "path = os.path.join(\"X-IIoTID dataset.csv\")  \n",
    "try:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    print(f\"✅ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: No se encontró el archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:06:56.378872Z",
     "iopub.status.busy": "2025-02-15T13:06:56.378598Z",
     "iopub.status.idle": "2025-02-15T13:07:12.369065Z",
     "shell.execute_reply": "2025-02-15T13:07:12.368043Z",
     "shell.execute_reply.started": "2025-02-15T13:06:56.378846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:12.371055Z",
     "iopub.status.busy": "2025-02-15T13:07:12.370633Z",
     "iopub.status.idle": "2025-02-15T13:07:14.347586Z",
     "shell.execute_reply": "2025-02-15T13:07:14.346847Z",
     "shell.execute_reply.started": "2025-02-15T13:07:12.371014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820834 entries, 0 to 820833\n",
      "Data columns (total 68 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   Date                         820503 non-null  object\n",
      " 1   Timestamp                    820537 non-null  object\n",
      " 2   Scr_IP                       820834 non-null  object\n",
      " 3   Scr_port                     820834 non-null  object\n",
      " 4   Des_IP                       820834 non-null  object\n",
      " 5   Des_port                     820834 non-null  object\n",
      " 6   Protocol                     820834 non-null  object\n",
      " 7   Service                      820834 non-null  object\n",
      " 8   Duration                     820834 non-null  object\n",
      " 9   Scr_bytes                    820834 non-null  object\n",
      " 10  Des_bytes                    820834 non-null  object\n",
      " 11  Conn_state                   820834 non-null  int64 \n",
      " 12  missed_bytes                 820834 non-null  object\n",
      " 13  is_syn_only                  820834 non-null  bool  \n",
      " 14  Is_SYN_ACK                   820834 non-null  bool  \n",
      " 15  is_pure_ack                  820834 non-null  bool  \n",
      " 16  is_with_payload              820834 non-null  bool  \n",
      " 17  FIN or RST                   820834 non-null  bool  \n",
      " 18  Bad_checksum                 820834 non-null  bool  \n",
      " 19  is_SYN_with_RST              820834 non-null  bool  \n",
      " 20  Scr_pkts                     820834 non-null  object\n",
      " 21  Scr_ip_bytes                 820834 non-null  object\n",
      " 22  Des_pkts                     820834 non-null  object\n",
      " 23  Des_ip_bytes                 820834 non-null  object\n",
      " 24  anomaly_alert                820834 non-null  object\n",
      " 25  total_bytes                  820834 non-null  object\n",
      " 26  total_packet                 820834 non-null  object\n",
      " 27  paket_rate                   820834 non-null  object\n",
      " 28  byte_rate                    820834 non-null  object\n",
      " 29  Scr_packts_ratio             820834 non-null  object\n",
      " 30  Des_pkts_ratio               820834 non-null  object\n",
      " 31  Scr_bytes_ratio              820834 non-null  object\n",
      " 32  Des_bytes_ratio              820834 non-null  object\n",
      " 33  Avg_user_time                820834 non-null  object\n",
      " 34  Std_user_time                820834 non-null  object\n",
      " 35  Avg_nice_time                820834 non-null  object\n",
      " 36  Std_nice_time                820834 non-null  object\n",
      " 37  Avg_system_time              820834 non-null  object\n",
      " 38  Std_system_time              820834 non-null  object\n",
      " 39  Avg_iowait_time              820834 non-null  object\n",
      " 40  Std_iowait_time              820834 non-null  object\n",
      " 41  Avg_ideal_time               820834 non-null  object\n",
      " 42  Std_ideal_time               820834 non-null  object\n",
      " 43  Avg_tps                      820834 non-null  object\n",
      " 44  Std_tps                      820834 non-null  object\n",
      " 45  Avg_rtps                     820834 non-null  object\n",
      " 46  Std_rtps                     820834 non-null  object\n",
      " 47  Avg_wtps                     820834 non-null  object\n",
      " 48  Std_wtps                     820834 non-null  object\n",
      " 49  Avg_ldavg_1                  820834 non-null  object\n",
      " 50  Std_ldavg_1                  820834 non-null  object\n",
      " 51  Avg_kbmemused                820834 non-null  object\n",
      " 52  Std_kbmemused                820834 non-null  object\n",
      " 53  Avg_num_Proc/s               820834 non-null  object\n",
      " 54  Std_num_proc/s               820834 non-null  object\n",
      " 55  Avg_num_cswch/s              820834 non-null  object\n",
      " 56  std_num_cswch/s              820834 non-null  object\n",
      " 57  OSSEC_alert                  820834 non-null  int64 \n",
      " 58  OSSEC_alert_level            820834 non-null  int64 \n",
      " 59  Login_attempt                820834 non-null  int64 \n",
      " 60  Succesful_login              820834 non-null  int64 \n",
      " 61  File_activity                820834 non-null  int64 \n",
      " 62  Process_activity             820834 non-null  int64 \n",
      " 63  read_write_physical.process  820834 non-null  int64 \n",
      " 64  is_privileged                820834 non-null  int64 \n",
      " 65  class1                       820834 non-null  object\n",
      " 66  class2                       820834 non-null  object\n",
      " 67  class3                       820834 non-null  object\n",
      "dtypes: bool(7), int64(9), object(52)\n",
      "memory usage: 387.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:14.349014Z",
     "iopub.status.busy": "2025-02-15T13:07:14.348703Z",
     "iopub.status.idle": "2025-02-15T13:07:18.246417Z",
     "shell.execute_reply": "2025-02-15T13:07:18.245401Z",
     "shell.execute_reply.started": "2025-02-15T13:07:14.348980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"Divide los datos en entrenamiento, validación y prueba.\"\"\"\n",
    "X = df.drop(columns=['class1', 'class2', 'class3'])\n",
    "y_class3 = df['class3'].map({'Normal': 0, 'Attack': 1})\n",
    "y_class2 = df['class2']\n",
    "y_class1 = df['class1']\n",
    "\n",
    "X_train, X_temp, y_train_class3, y_temp_class3, y_train_class2, y_temp_class2, y_train_class1, y_temp_class1 = train_test_split(\n",
    "    X, y_class3, y_class2, y_class1, test_size=0.3, random_state=random_state, stratify=y_class3, shuffle=True\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val_class3, y_test_class3, y_val_class2, y_test_class2, y_val_class1, y_test_class1 = train_test_split(\n",
    "    X_temp, y_temp_class3, y_temp_class2, y_temp_class1, test_size=0.5, random_state=random_state, stratify=y_temp_class3, shuffle=True\n",
    ")\n",
    "\n",
    "# Resetear índices para evitar desalineaciones\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class3 = y_train_class3.reset_index(drop=True)\n",
    "y_val_class3 = y_val_class3.reset_index(drop=True)\n",
    "y_test_class3 = y_test_class3.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class2 = y_train_class2.reset_index(drop=True)\n",
    "y_val_class2 = y_val_class2.reset_index(drop=True)\n",
    "y_test_class2 = y_test_class2.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class1 = y_train_class1.reset_index(drop=True)\n",
    "y_val_class1 = y_val_class1.reset_index(drop=True)\n",
    "y_test_class1 = y_test_class1.reset_index(drop=True)  # Opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.247505Z",
     "iopub.status.busy": "2025-02-15T13:07:18.247244Z",
     "iopub.status.idle": "2025-02-15T13:07:18.254487Z",
     "shell.execute_reply": "2025-02-15T13:07:18.253506Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.247468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def fix_dtype(df, umbral_numerico=0.7):\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "\n",
    "    # Convertir booleanos a float\n",
    "    df[bool_cols] = df[bool_cols].astype(float)\n",
    "\n",
    "    for col in object_cols:\n",
    "        valores_unicos = df[col].dropna().unique()\n",
    "\n",
    "        if {\"true\", \"false\"} <= set(valores_unicos):  # Verifica si ambos existen\n",
    "            df[col] = df[col].map({'true': 1, 'false': 0}).astype(float)\n",
    "        else:\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            if converted.notna().mean() > umbral_numerico:\n",
    "                df[col] = converted.astype(float)\n",
    "\n",
    "    for col in int_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def delete_ip_port(df):\n",
    "    \"\"\"Elimina las columnas 'ip' y 'port'.\"\"\"\n",
    "    lista = ['Scr_IP', 'Scr_port', 'Des_IP', 'Des_port', 'Scr_bytes', 'Des_bytes', 'Scr_pkts', \n",
    "                            'Des_pkts', 'Scr_ip_bytes', 'Des_ip_bytes', 'Scr_packts_ratio', 'Des_pkts_ratio',\n",
    "                            'Scr_bytes_ratio', 'Des_bytes_ratio']\n",
    "\n",
    "    return df.drop(columns=lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.255763Z",
     "iopub.status.busy": "2025-02-15T13:07:18.255503Z",
     "iopub.status.idle": "2025-02-15T13:07:18.270150Z",
     "shell.execute_reply": "2025-02-15T13:07:18.269396Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.255741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Reemplazos comunes de valores\n",
    "common_replacements = {\n",
    "    '-': np.nan,\n",
    "    '?': np.nan,\n",
    "    'nan': np.nan,\n",
    "}\n",
    "\n",
    "def replace_common_values(df):\n",
    "    \"\"\"Reemplaza valores comunes como '-', '?' y 'nan' por NaN.\"\"\"\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].replace(common_replacements)\n",
    "    return df\n",
    "\n",
    "def fix_mayus(df):\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.271536Z",
     "iopub.status.busy": "2025-02-15T13:07:18.271215Z",
     "iopub.status.idle": "2025-02-15T13:07:18.283357Z",
     "shell.execute_reply": "2025-02-15T13:07:18.282420Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.271506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Definir los imputadores\n",
    "imputers = {\n",
    "    'categorical': {\n",
    "        'most_frequent': SimpleImputer(strategy='most_frequent'),\n",
    "        'knn': KNNImputer(n_neighbors=5)\n",
    "    },\n",
    "    'numeric': {\n",
    "        'mean': SimpleImputer(strategy='mean'),\n",
    "        'median': SimpleImputer(strategy='median')\n",
    "    }\n",
    "}\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Definir escaladores\n",
    "scalers = {\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"standard\": StandardScaler()\n",
    "}\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Definir codificadores\n",
    "encoders = {\n",
    "    \"one_hot\": OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.285919Z",
     "iopub.status.busy": "2025-02-15T13:07:18.285663Z",
     "iopub.status.idle": "2025-02-15T13:07:18.298219Z",
     "shell.execute_reply": "2025-02-15T13:07:18.297449Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.285899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imputador_cat= imputers['categorical']['most_frequent']\n",
    "imputador_num = imputers['numeric']['mean']\n",
    "normalizacion = scalers['robust']\n",
    "decodificador = encoders['one_hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.299977Z",
     "iopub.status.busy": "2025-02-15T13:07:18.299660Z",
     "iopub.status.idle": "2025-02-15T13:07:18.312358Z",
     "shell.execute_reply": "2025-02-15T13:07:18.311667Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.299925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "def matriz_correlacion(df):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    return correlation_matrix\n",
    "\n",
    "def correlacion_pares(df, umbral):\n",
    "    df = matriz_correlacion(df)\n",
    "    # Toma solo la parte superior de la matriz para evitar duplicados\n",
    "    upper_tri = df.where(np.triu(np.ones(df.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identifica pares altamente correlacionados\n",
    "    correlated_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        for row in upper_tri.index:\n",
    "            if upper_tri.loc[row, col] > umbral:\n",
    "                correlated_pairs.append((row, col))\n",
    "\n",
    "    # Selecciona las columnas a eliminar (de cada par, se elimina la que aparece como columna)\n",
    "    alta_corr_pares = [col for col in upper_tri.columns if any(upper_tri[col] > umbral)]\n",
    "    \n",
    "    return alta_corr_pares\n",
    "\n",
    "def correlacion_respecto_objetivo(df, target, umbral):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Calculamos la correlación con la variable objetivo\n",
    "    target_correlation = df[numeric_cols].corrwith(target).abs().sort_values(ascending=True)\n",
    "\n",
    "    # Nos quedamos solo con las características que tengan correlación >= 0.1\n",
    "    baja_corr_respecto_obj = target_correlation[target_correlation < umbral].index.tolist()\n",
    "\n",
    "    return baja_corr_respecto_obj\n",
    "\n",
    "def seleccionar_variables_pca(X_train, X_val, n_components, num_top_features):\n",
    "    \"\"\"\n",
    "    Aplica PCA para seleccionar las características más influyentes, pero mantiene los datos originales.\n",
    "    \n",
    "    Parámetros:\n",
    "        - X_train: DataFrame de entrenamiento\n",
    "        - X_val: DataFrame de validación\n",
    "        - n_components: float/int, cantidad de componentes principales o porcentaje de varianza a retener\n",
    "        - num_top_features: int, número de características más influyentes a seleccionar\n",
    "\n",
    "    Retorna:\n",
    "        - X_train_filtrado: DataFrame de entrenamiento con las características seleccionadas\n",
    "        - X_val_filtrado: DataFrame de validación con las características seleccionadas\n",
    "    \"\"\"\n",
    "\n",
    "    # Si usas el sample, cambialo en las lineas necesarias\n",
    "    # X_train_sample = X_train.sample(n=300000, random_state=42) # Seleccionar una muestra de 300,000 instancias como en el articulo\n",
    "    \n",
    "    # Aplicar PCA (sin guardar la transformación)\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    pca.fit(X_train)  # Solo ajustamos el modelo, no transformamos los datos\n",
    "\n",
    "    # Obtener nombres originales de las variables\n",
    "    original_feature_names = np.array(X_train.columns)\n",
    "\n",
    "    # Contador de importancia de características en PCA\n",
    "    feature_counter = Counter()\n",
    "    \n",
    "    for comp in pca.components_:\n",
    "        top_indices = np.argsort(np.abs(comp))[-num_top_features:]  # Índices de las más importantes\n",
    "        top_features = original_feature_names[top_indices]  # Obtener nombres\n",
    "        feature_counter.update(top_features)  # Contar ocurrencias\n",
    "\n",
    "    # Seleccionar las variables más influyentes ordenadas por frecuencia de aparición\n",
    "    variables_pca = [feature for feature, _ in feature_counter.most_common()]\n",
    "\n",
    "    # Filtrar las variables seleccionadas en los conjuntos de datos\n",
    "    X_train_filtrado = X_train[variables_pca]\n",
    "    X_val_filtrado = X_val[variables_pca]\n",
    "    print(\"Características seleccionadas por PCA:\", variables_pca)\n",
    "    return X_train_filtrado, X_val_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.313324Z",
     "iopub.status.busy": "2025-02-15T13:07:18.313109Z",
     "iopub.status.idle": "2025-02-15T13:07:18.344286Z",
     "shell.execute_reply": "2025-02-15T13:07:18.343442Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.313305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculo_varianza(df):\n",
    "    \"\"\"Calcula las varianzas de las columnas de un DataFrame y devuelve las que tienen varianza igual a cero.\"\"\"\n",
    "    varianzas = df.var()\n",
    "\n",
    "    # Identificar columnas con varianza igual a cero\n",
    "    variables_con_varianza_cero = [col for col, varianza in varianzas.items() if varianza == 0]\n",
    "    \n",
    "    return variables_con_varianza_cero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:18.345339Z",
     "iopub.status.busy": "2025-02-15T13:07:18.345052Z",
     "iopub.status.idle": "2025-02-15T13:07:52.960992Z",
     "shell.execute_reply": "2025-02-15T13:07:52.959839Z",
     "shell.execute_reply.started": "2025-02-15T13:07:18.345310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = replace_common_values(X_train)\n",
    "X_train = fix_mayus(X_train)\n",
    "X_train = fix_dtype(X_train)\n",
    "X_train = delete_ip_port(X_train)\n",
    "\n",
    "y_train_class3 = y_train_class3.loc[X_train.index]\n",
    "y_train_class2 = y_train_class2.loc[X_train.index]\n",
    "y_train_class1 = y_train_class1.loc[X_train.index]\n",
    "\n",
    "X_val = replace_common_values(X_val)\n",
    "X_val = fix_mayus(X_val)\n",
    "X_val = fix_dtype(X_val)\n",
    "X_val = delete_ip_port(X_val)\n",
    "\n",
    "y_val_class3 = y_val_class3.loc[X_val.index]\n",
    "y_val_class2 = y_val_class2.loc[X_val.index]\n",
    "y_val_class1 = y_val_class1.loc[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:52.962534Z",
     "iopub.status.busy": "2025-02-15T13:07:52.962175Z",
     "iopub.status.idle": "2025-02-15T13:07:58.129406Z",
     "shell.execute_reply": "2025-02-15T13:07:58.128580Z",
     "shell.execute_reply.started": "2025-02-15T13:07:52.962497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instancias completas: 416933, incompletas: 157650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train['Instancia_completa'] = X_train.notnull().all(axis=1).astype(int)\n",
    "X_val['Instancia_completa'] = X_val.notnull().all(axis=1).astype(int)\n",
    "\n",
    "completas = X_train['Instancia_completa'].sum()\n",
    "incompletas = len(X_train) - completas\n",
    "print(f\"✅ Instancias completas: {completas}, incompletas: {incompletas}\")\n",
    "sample_weight_train = X_train['Instancia_completa'].replace({1: 3, 0: 1})\n",
    "\n",
    "columnas_no_comprobar = [col for col in X_train.columns if col not in ['Timestamp', 'Date', 'Instancia_completa'] and X_train[col].dtypes != 'object']\n",
    "variables_con_varianza_cero = calculo_varianza(X_train[columnas_no_comprobar])\n",
    "X_train = X_train.drop(columns=variables_con_varianza_cero)\n",
    "X_val = X_val.drop(columns=variables_con_varianza_cero)\n",
    "\n",
    "X_train = X_train.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\n",
    "X_val = X_val.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\n",
    "\n",
    "alta_corr_pares = correlacion_pares(X_train, 0.97)\n",
    "X_train = X_train.drop(columns=alta_corr_pares)\n",
    "X_val = X_val.drop(columns=alta_corr_pares)\n",
    "\n",
    "baja_corr_respecto_obj = correlacion_respecto_objetivo(X_train, y_train_class3, 0.025)\n",
    "X_train = X_train.drop(columns=baja_corr_respecto_obj)\n",
    "X_val = X_val.drop(columns=baja_corr_respecto_obj)\n",
    "\n",
    "caracteritisticas_seleccionadas = X_train.columns.tolist()\n",
    "\n",
    "X_train['Protocol'] = X_train['Protocol'].fillna(\"missing\")\n",
    "X_val['Protocol'] = X_val['Protocol'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:07:58.130750Z",
     "iopub.status.busy": "2025-02-15T13:07:58.130390Z",
     "iopub.status.idle": "2025-02-15T13:08:00.929074Z",
     "shell.execute_reply": "2025-02-15T13:08:00.928208Z",
     "shell.execute_reply.started": "2025-02-15T13:07:58.130716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Identificar columnas categóricas, numéricas y booleanas\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "boolean_cols = X_train.select_dtypes(include=['bool']).columns\n",
    "if boolean_cols.any():  # Si hay columnas booleanas\n",
    "    X_train[boolean_cols] = X_train[boolean_cols].astype(float)  # TAL VEZ INNCESESARIO\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "##############################################################################\n",
    "    \n",
    "X_train[categorical_cols] = imputador_cat.fit_transform(X_train[categorical_cols])\n",
    "X_val[categorical_cols] = imputador_cat.transform(X_val[categorical_cols])\n",
    "\n",
    "X_train[numerical_cols] = imputador_num.fit_transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = imputador_num.transform(X_val[numerical_cols])\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train_scaled = normalizacion.fit_transform(X_train[numerical_cols])\n",
    "X_val_scaled = normalizacion.transform(X_val[numerical_cols])\n",
    "\n",
    "# Convertir las matrices escaladas a DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_train.index)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_val.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train_encoded = decodificador.fit_transform(X_train[categorical_cols])\n",
    "X_val_encoded = decodificador.transform(X_val[categorical_cols])\n",
    "\n",
    "# Obtener los nombres de las nuevas columnas codificadas\n",
    "encoded_cols = decodificador.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Convertir las matrices codificadas a DataFrames\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index)\n",
    "X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_cols, index=X_val.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Combinar con las características categóricas codificadas\n",
    "X_train_processed = pd.concat([X_train_scaled_df, X_train_encoded_df], axis=1)\n",
    "X_val_processed = pd.concat([X_val_scaled_df, X_val_encoded_df], axis=1)\n",
    "\n",
    "# Opcional: Reordenar las columnas si es necesario\n",
    "X_train = X_train_processed.reindex(sorted(X_train_processed.columns), axis=1)\n",
    "X_val = X_val_processed.reindex(sorted(X_val_processed.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:00.930124Z",
     "iopub.status.busy": "2025-02-15T13:08:00.929877Z",
     "iopub.status.idle": "2025-02-15T13:08:02.165343Z",
     "shell.execute_reply": "2025-02-15T13:08:02.164527Z",
     "shell.execute_reply.started": "2025-02-15T13:08:00.930104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características seleccionadas por PCA: ['is_syn_only_scaled', 'Avg_ideal_time_scaled', 'Avg_system_time_scaled', 'std_num_cswch/s_scaled', 'paket_rate_scaled', 'Avg_num_cswch/s_scaled', 'total_bytes_scaled', 'Protocol_tcp', 'Is_SYN_ACK_scaled', 'Std_user_time_scaled', 'Avg_nice_time_scaled', 'Std_system_time_scaled', 'read_write_physical.process_scaled', 'Avg_user_time_scaled', 'Avg_kbmemused_scaled', 'Duration_scaled', 'Protocol_udp', 'Service_coap', 'Service_dns', 'Avg_iowait_time_scaled', 'total_packet_scaled', 'Avg_tps_scaled', 'Service_http', 'Avg_ldavg_1_scaled', 'Std_nice_time_scaled', 'Service_websocket', 'OSSEC_alert_scaled', 'Avg_rtps_scaled', 'Service_mqtt', 'is_with_payload_scaled', 'anomaly_alert_scaled', 'File_activity_scaled', 'Login_attempt_scaled', 'Std_wtps_scaled']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = seleccionar_variables_pca(X_train_processed, X_val_processed, n_components=0.95, num_top_features=20)\n",
    "caracteritisticas_procesadas = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:02.166427Z",
     "iopub.status.busy": "2025-02-15T13:08:02.166220Z",
     "iopub.status.idle": "2025-02-15T13:08:02.251506Z",
     "shell.execute_reply": "2025-02-15T13:08:02.250591Z",
     "shell.execute_reply.started": "2025-02-15T13:08:02.166408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574583, 34) (123125, 34)\n",
      "0 0\n",
      "class3\n",
      "0    0.5134\n",
      "1    0.4866\n",
      "Name: proportion, dtype: float64\n",
      "class3\n",
      "0    0.513405\n",
      "1    0.486595\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)\n",
    "print(X_train.isnull().sum().sum(), X_val.isnull().sum().sum())\n",
    "print(y_train_class3.value_counts(normalize=True))\n",
    "print(y_val_class3.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:02.252802Z",
     "iopub.status.busy": "2025-02-15T13:08:02.252446Z",
     "iopub.status.idle": "2025-02-15T13:08:02.483682Z",
     "shell.execute_reply": "2025-02-15T13:08:02.482760Z",
     "shell.execute_reply.started": "2025-02-15T13:08:02.252755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder_class3 = LabelEncoder()\n",
    "y_train_class3 = label_encoder_class3.fit_transform(y_train_class3)\n",
    "y_val_class3   = label_encoder_class3.transform(y_val_class3)\n",
    "y_test_class3  = label_encoder_class3.transform(y_test_class3)\n",
    "\n",
    "mapping_class3 = dict(enumerate(label_encoder_class3.classes_))\n",
    "\n",
    "\n",
    "label_encoder_class2 = LabelEncoder()\n",
    "y_train_class2 = label_encoder_class2.fit_transform(y_train_class2)\n",
    "y_val_class2   = label_encoder_class2.transform(y_val_class2)\n",
    "y_test_class2  = label_encoder_class2.transform(y_test_class2)\n",
    "\n",
    "mapping_class2 = dict(enumerate(label_encoder_class2.classes_))\n",
    "\n",
    "label_encoder_class1 = LabelEncoder()\n",
    "y_train_class1 = label_encoder_class1.fit_transform(y_train_class1)\n",
    "y_val_class1   = label_encoder_class1.transform(y_val_class1)\n",
    "y_test_class1  = label_encoder_class1.transform(y_test_class1)\n",
    "\n",
    "mapping_class1 = dict(enumerate(label_encoder_class1.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "{0: 'C&C', 1: 'Exfiltration', 2: 'Exploitation', 3: 'Lateral _movement', 4: 'Normal', 5: 'RDOS', 6: 'Reconnaissance', 7: 'Tampering', 8: 'Weaponization', 9: 'crypto-ransomware'}\n",
      "{0: 'BruteForce', 1: 'C&C', 2: 'Dictionary', 3: 'Discovering_resources', 4: 'Exfiltration', 5: 'Fake_notification', 6: 'False_data_injection', 7: 'Generic_scanning', 8: 'MQTT_cloud_broker_subscription', 9: 'MitM', 10: 'Modbus_register_reading', 11: 'Normal', 12: 'RDOS', 13: 'Reverse_shell', 14: 'Scanning_vulnerability', 15: 'TCP Relay', 16: 'crypto-ransomware', 17: 'fuzzing', 18: 'insider_malcious'}\n"
     ]
    }
   ],
   "source": [
    "print(mapping_class3)\n",
    "print(mapping_class2)\n",
    "print(mapping_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:02.484880Z",
     "iopub.status.busy": "2025-02-15T13:08:02.484567Z",
     "iopub.status.idle": "2025-02-15T13:08:02.571592Z",
     "shell.execute_reply": "2025-02-15T13:08:02.570630Z",
     "shell.execute_reply.started": "2025-02-15T13:08:02.484848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Número de características de entrada\n",
    "num_classes_2 = len(set(y_train_class2))  # Cantidad de clases en y_class2\n",
    "num_classes_1 = len(set(y_train_class1))  # Cantidad de clases en y_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:02.572604Z",
     "iopub.status.busy": "2025-02-15T13:08:02.572395Z",
     "iopub.status.idle": "2025-02-15T13:08:02.699061Z",
     "shell.execute_reply": "2025-02-15T13:08:02.698388Z",
     "shell.execute_reply.started": "2025-02-15T13:08:02.572585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "timesteps = 1\n",
    "# Convertir DataFrame a NumPy antes de redimensionar\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0], timesteps, X_train.shape[1]))\n",
    "X_val = np.array(X_val).reshape((X_val.shape[0], timesteps, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:08:02.700033Z",
     "iopub.status.busy": "2025-02-15T13:08:02.699778Z",
     "iopub.status.idle": "2025-02-15T13:11:02.134430Z",
     "shell.execute_reply": "2025-02-15T13:11:02.133682Z",
     "shell.execute_reply.started": "2025-02-15T13:08:02.700011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - accuracy: 0.9575 - loss: 0.1276 - val_accuracy: 0.9789 - val_loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_class1.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Entrada\n",
    "input_layer = keras.Input(shape=(timesteps, input_dim))  # Se necesita un input secuencial\n",
    "\n",
    "# Capas ocultas con GRU\n",
    "x = layers.GRU(200, return_sequences=True)(input_layer)\n",
    "x = layers.GRU(200, return_sequences=True)(x)\n",
    "x = layers.GRU(200)(x)  # Última capa sin return_sequences\n",
    "\n",
    "# Salidas\n",
    "output_class3 = layers.Dense(1, activation='sigmoid', name=\"output_class3\")(x)  # Binaria\n",
    "\n",
    "# Modelo con dos salidas\n",
    "model_class1 = keras.Model(inputs=input_layer, outputs=output_class3)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_class1.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir el callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_class1.fit(X_train,\n",
    "                    y_train_class3,\n",
    "                    batch_size=250,\n",
    "                    epochs=1,\n",
    "                    validation_data=(X_val, y_val_class3),\n",
    "                   callbacks=[early_stopping])\n",
    "\n",
    "# Guardar el modelo con un nombre único\n",
    "model_name = f\"modelo_class1\"\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "carpeta_modelos = \"modelos_cascada_gru\"\n",
    "os.makedirs(carpeta_modelos, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo dentro de la carpeta\n",
    "ruta_modelo = os.path.join(carpeta_modelos, f\"{model_name}.h5\")\n",
    "model_class1.save(ruta_modelo)\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_modelo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:11:02.136092Z",
     "iopub.status.busy": "2025-02-15T13:11:02.135756Z",
     "iopub.status.idle": "2025-02-15T13:11:13.871414Z",
     "shell.execute_reply": "2025-02-15T13:11:13.870513Z",
     "shell.execute_reply.started": "2025-02-15T13:11:02.136066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3848/3848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step\n",
      "(123125, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase3\n",
    "y_pred_class3 = model_class1.predict(X_val)\n",
    "y_pred_class3 = (y_pred_class3 >= 0.5).astype(int)\n",
    "print(y_pred_class3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:11:13.872914Z",
     "iopub.status.busy": "2025-02-15T13:11:13.872554Z",
     "iopub.status.idle": "2025-02-15T13:11:24.881135Z",
     "shell.execute_reply": "2025-02-15T13:11:24.880201Z",
     "shell.execute_reply.started": "2025-02-15T13:11:13.872870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "indices_train = np.where(y_train_class3 == 1)[0]\n",
    "X_train = X_train[indices_train]\n",
    "y_train_class2 = y_train_class2[indices_train]\n",
    "y_train_class2 = y_train_class2.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58217, 1, 34) (58217,)\n"
     ]
    }
   ],
   "source": [
    "indices_val = np.where(y_pred_class3 == 1)[0]\n",
    "X_val = X_val[indices_val]\n",
    "y_val_class2 = y_val_class2[indices_val].ravel()\n",
    "\n",
    "print(X_val.shape, y_val_class2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 19ms/step - accuracy: 0.9390 - loss: 0.2109 - val_accuracy: 0.9871 - val_loss: 0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_class2.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Entrada\n",
    "input_layer = keras.Input(shape=(timesteps, input_dim))  # Se necesita un input secuencial\n",
    "\n",
    "# Capas ocultas con GRU\n",
    "x = layers.GRU(200, return_sequences=True)(input_layer)\n",
    "x = layers.GRU(200, return_sequences=True)(x)\n",
    "x = layers.GRU(200)(x)  # Última capa sin return_sequences\n",
    "\n",
    "# Salidas\n",
    "output_class2 = layers.Dense(num_classes_2, activation='softmax', name=\"output_class2\")(x)  # Binaria\n",
    "\n",
    "# Modelo con dos salidas\n",
    "model_class2= keras.Model(inputs=input_layer, outputs=output_class2)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_class2.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir el callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_class2.fit(X_train,\n",
    "                    y_train_class2,\n",
    "                    batch_size=250,\n",
    "                    epochs=1,\n",
    "                    validation_data=(X_val, y_val_class2),\n",
    "                   callbacks=[early_stopping])\n",
    "\n",
    "# Guardar el modelo con un nombre único\n",
    "model_name = f\"modelo_class2\"\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "carpeta_modelos = \"modelos_cascada_gru\"\n",
    "os.makedirs(carpeta_modelos, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo dentro de la carpeta\n",
    "ruta_modelo = os.path.join(carpeta_modelos, f\"{model_name}.h5\")\n",
    "model_class1.save(ruta_modelo)\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_modelo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "[3 6 8 ... 8 5 5]\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase2\n",
    "y_pred_class2 = model_class2.predict(X_val)\n",
    "y_pred_class2 = np.argmax(y_pred_class2, axis=1)\n",
    "print(y_pred_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class1 = y_train_class1[indices_train].ravel()\n",
    "y_val_class1 = y_val_class1[indices_val].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - accuracy: 0.4588 - loss: 2.6648 - val_accuracy: 0.9224 - val_loss: 0.7409\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8948 - loss: 0.8536 - val_accuracy: 0.9224 - val_loss: 0.4805\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8909 - loss: 0.4860 - val_accuracy: 0.9224 - val_loss: 0.5520\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8935 - loss: 0.3371 - val_accuracy: 0.9224 - val_loss: 0.6255\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9009 - loss: 0.2487 - val_accuracy: 0.9224 - val_loss: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_Exploitation.h5\n",
      "**************************************************\n",
      "Epoch 1/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8722 - loss: 0.7564 - val_accuracy: 0.9722 - val_loss: 0.2509\n",
      "Epoch 2/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9949 - loss: 0.0187 - val_accuracy: 0.9724 - val_loss: 0.2807\n",
      "Epoch 3/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0080 - val_accuracy: 0.9724 - val_loss: 0.2959\n",
      "Epoch 4/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0057 - val_accuracy: 0.9722 - val_loss: 0.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_Lateral _movement.h5\n",
      "**************************************************\n",
      "Epoch 1/10\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 31ms/step - accuracy: 0.8264 - loss: 0.5394 - val_accuracy: 0.9524 - val_loss: 0.3638\n",
      "Epoch 2/10\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9842 - loss: 0.0542 - val_accuracy: 0.9714 - val_loss: 0.3671\n",
      "Epoch 3/10\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9963 - loss: 0.0137 - val_accuracy: 0.9713 - val_loss: 0.4013\n",
      "Epoch 4/10\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0078 - val_accuracy: 0.9714 - val_loss: 0.4243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_Reconnaissance.h5\n",
      "**************************************************\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.8104 - loss: 1.5594 - val_accuracy: 0.9890 - val_loss: 0.0868\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9947 - loss: 0.0388 - val_accuracy: 0.9890 - val_loss: 0.0899\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9954 - loss: 0.0257 - val_accuracy: 0.9890 - val_loss: 0.0973\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9943 - loss: 0.0242 - val_accuracy: 0.9890 - val_loss: 0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_Tampering.h5\n",
      "**************************************************\n",
      "Epoch 1/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9206 - loss: 0.3933 - val_accuracy: 0.9978 - val_loss: 0.0246\n",
      "Epoch 2/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 3.2830e-04 - val_accuracy: 0.9978 - val_loss: 0.0260\n",
      "Epoch 3/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 1.1112e-04 - val_accuracy: 0.9978 - val_loss: 0.0270\n",
      "Epoch 4/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 6.0044e-05 - val_accuracy: 0.9978 - val_loss: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_cascada_gru\\modelo_Weaponization.h5\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "modelos = []  # Lista para almacenar los modelos\n",
    "\n",
    "class_names = np.unique(y_train_class2)\n",
    "for name in class_names:\n",
    "\n",
    "    indices_train_cat = np.where(y_train_class2 == name)[0]\n",
    "    X_train_cat = X_train[indices_train_cat]\n",
    "    y_train_class1_cat = y_train_class1[indices_train_cat]\n",
    "    \n",
    "    if np.unique(y_train_class1_cat).size == 1:\n",
    "        continue  \n",
    "\n",
    "    indices_val_cat = np.where(y_pred_class2 == name)[0]\n",
    "    X_val_cat = X_val[indices_val_cat]\n",
    "    y_val_class1_cat = y_val_class1[indices_val_cat]\n",
    "    \n",
    "    # Entrada\n",
    "    input_layer = keras.Input(shape=(timesteps, input_dim))  # Se necesita un input secuencial\n",
    "\n",
    "    # Capas ocultas con GRU\n",
    "    x = layers.GRU(200, return_sequences=True)(input_layer)\n",
    "    x = layers.GRU(200, return_sequences=True)(x)\n",
    "    x = layers.GRU(200)(x)  # Última capa sin return_sequences\n",
    "\n",
    "    # Salidas\n",
    "    output_class1 = layers.Dense(num_classes_1, activation='softmax', name=\"output_class1\")(x)  # Binaria\n",
    "\n",
    "    # Modelo con dos salidas\n",
    "    model= keras.Model(inputs=input_layer, outputs=output_class1)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Definir el callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(X_train_cat,\n",
    "                        y_train_class1_cat,\n",
    "                        batch_size=250,\n",
    "                        epochs=10,\n",
    "                        validation_data=(X_val_cat, y_val_class1_cat),\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    # Guardar el modelo con un nombre único\n",
    "    nombre = mapping_class2[name]\n",
    "    model_name = f\"modelo_{nombre}\"\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    carpeta_modelos = \"modelos_cascada_gru\"\n",
    "    os.makedirs(carpeta_modelos, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo dentro de la carpeta\n",
    "    ruta_modelo = os.path.join(carpeta_modelos, f\"{model_name}.h5\")\n",
    "    model.save(ruta_modelo)\n",
    "\n",
    "    print(f\"Modelo guardado en: {ruta_modelo}\")\n",
    "\n",
    "    \n",
    "    # Añadir el modelo a la lista\n",
    "    modelos.append((model_name, model))  # Almacena el nombre y el modelo\n",
    "    \n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "Predicciones para modelo_Exploitation:\n",
      "[13 13 13 ... 13 13 13]\n",
      "Predicciones para modelo_Lateral _movement:\n",
      "[10 15 15 ... 15 10 10]\n",
      "Predicciones para modelo_Reconnaissance:\n",
      "[17 14 14 ... 14 14  3]\n",
      "Predicciones para modelo_Tampering:\n",
      "[ 6  6  6 ...  6 13 14]\n",
      "Predicciones para modelo_Weaponization:\n",
      "[2 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predicciones = {}  # Diccionario para almacenar las predicciones de cada modelo\n",
    "\n",
    "for model_name, model in modelos:\n",
    "    y_pred_class2 = model.predict(X_val)\n",
    "    y_pred_class2 = np.argmax(y_pred_class2, axis=1)\n",
    "    \n",
    "    # Guardar las predicciones en un diccionario con el nombre del modelo\n",
    "    predicciones[model_name] = y_pred_class2\n",
    "\n",
    "# Mostrar las predicciones de cada modelo\n",
    "for name, pred in predicciones.items():\n",
    "    print(f\"Predicciones para {name}:\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:11:24.882339Z",
     "iopub.status.busy": "2025-02-15T13:11:24.882044Z",
     "iopub.status.idle": "2025-02-15T13:11:30.820418Z",
     "shell.execute_reply": "2025-02-15T13:11:30.819443Z",
     "shell.execute_reply.started": "2025-02-15T13:11:24.882314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test = replace_common_values(X_test)\n",
    "X_test = fix_mayus(X_test)\n",
    "X_test = fix_dtype(X_test)\n",
    "X_test = delete_ip_port(X_test)\n",
    "\n",
    "y_test_class3 = y_test_class3[X_test.index]\n",
    "\n",
    "X_test = X_test[caracteritisticas_seleccionadas] \n",
    "\n",
    "X_test['Protocol'] = X_test['Protocol'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:11:30.824496Z",
     "iopub.status.busy": "2025-02-15T13:11:30.824205Z",
     "iopub.status.idle": "2025-02-15T13:11:31.112066Z",
     "shell.execute_reply": "2025-02-15T13:11:31.111149Z",
     "shell.execute_reply.started": "2025-02-15T13:11:30.824470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas, numéricas y booleanas\n",
    "categorical_cols = X_test.select_dtypes(include=['object']).columns\n",
    "boolean_cols = X_test.select_dtypes(include=['bool']).columns\n",
    "if boolean_cols.any():  # Si hay columnas booleanas\n",
    "    X_test[boolean_cols] = X_test[boolean_cols].astype(float) # TAL VEZ INNCESESARIO\n",
    "numerical_cols = X_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test[categorical_cols] = imputador_cat.transform(X_test[categorical_cols])\n",
    "\n",
    "X_test[numerical_cols] = imputador_num.transform(X_test[numerical_cols])\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test_scaled = normalizacion.transform(X_test[numerical_cols])\n",
    "\n",
    "# Convertir las matrices escaladas a DataFrames\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_test.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test_encoded = decodificador.transform(X_test[categorical_cols])\n",
    "\n",
    "# Obtener los nombres de las nuevas columnas codificadas\n",
    "encoded_cols = decodificador.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Convertir las matrices codificadas a DataFrames\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_cols, index=X_test.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Combinar con las características categóricas codificadas\n",
    "X_test_processed = pd.concat([X_test_scaled_df, X_test_encoded_df], axis=1)\n",
    "\n",
    "# Opcional: Reordenar las columnas si es necesario\n",
    "X_test_processed = X_test_processed.reindex(sorted(X_test_processed.columns), axis=1)\n",
    "\n",
    "X_test = X_test_processed[caracteritisticas_procesadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T13:11:31.113589Z",
     "iopub.status.busy": "2025-02-15T13:11:31.113262Z",
     "iopub.status.idle": "2025-02-15T13:11:31.131870Z",
     "shell.execute_reply": "2025-02-15T13:11:31.130811Z",
     "shell.execute_reply.started": "2025-02-15T13:11:31.113563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convertir DataFrame a NumPy antes de redimensionar\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0], timesteps, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3848/3848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase3\n",
    "y_pred_class3 = model_class1.predict(X_test)\n",
    "y_pred_class3 = (y_pred_class3 >= 0.5).astype(int)\n",
    "print(y_pred_class3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_class3, y_pred_class3)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_test = np.where(y_pred_class3 == 1)[0]\n",
    "X_test = X_test[indices_test]\n",
    "y_test_class2 = y_test_class2[indices_test].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1820/1820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase2\n",
    "y_pred_class2 = model_class2.predict(X_test)\n",
    "y_pred_class2 = np.argmax(y_pred_class2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_class2, y_pred_class2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class1 = y_test_class1[indices_test].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de nombres a números: {'C&C': 0, 'Exfiltration': 1, 'Exploitation': 2, 'Lateral _movement': 3, 'Normal': 4, 'RDOS': 5, 'Reconnaissance': 6, 'Tampering': 7, 'Weaponization': 8, 'crypto-ransomware': 9}\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario con {nombre: número de clase}\n",
    "class_name_to_number = {value: key for key, value in mapping_class2.items()}\n",
    "\n",
    "print(\"Mapeo de nombres a números:\", class_name_to_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase Exploitation, Valores únicos en y_test_class1_cat: (array([ 5, 11, 13, 15]), array([ 2,  1, 88,  6], dtype=int64))\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step\n",
      "Accuracy para Exploitation: 0.9072\n",
      "**************************************************\n",
      "\n",
      "Clase Lateral _movement, Valores únicos en y_test_class1_cat: (array([ 6,  7,  8, 10, 11, 13, 14, 15, 16]), array([   8,    8, 3361,  840,   88,   14,    2,  212,    1], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy para Lateral _movement: 0.9711\n",
      "**************************************************\n",
      "\n",
      "Clase Reconnaissance, Valores únicos en y_test_class1_cat: (array([ 0,  1,  2,  3,  4,  7,  8, 10, 11, 12, 13, 14, 15, 17]), array([  31,    5,    9, 1987,    2, 7509,  162,    3,  268,    6,   19,\n",
      "       7907,   12,   59], dtype=int64))\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "Accuracy para Reconnaissance: 0.9537\n",
      "**************************************************\n",
      "\n",
      "Clase Tampering, Valores únicos en y_test_class1_cat: (array([6, 8]), array([722,  10], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy para Tampering: 0.9863\n",
      "**************************************************\n",
      "\n",
      "Clase Weaponization, Valores únicos en y_test_class1_cat: (array([ 0,  2,  5,  8, 11, 14, 18]), array([7121,  364,    1,    1,   10,    3, 2638], dtype=int64))\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Accuracy para Weaponization: 0.9985\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class_names = ['Exploitation', 'Lateral _movement', 'Reconnaissance', 'Tampering', 'Weaponization']\n",
    "\n",
    "for name in class_names:\n",
    "    # Obtener el número de clase correspondiente al nombre\n",
    "    if name not in class_name_to_number:\n",
    "        print(f\"Clase {name} no encontrada en el mapeo, saltando...\")\n",
    "        continue  \n",
    "\n",
    "    class_number = class_name_to_number[name]\n",
    "\n",
    "    # Filtrar índices de la clase\n",
    "    indices_test_cat = np.where(y_pred_class2 == class_number)[0]\n",
    "\n",
    "    # Verificar si hay datos antes de continuar\n",
    "    if len(indices_test_cat) == 0:\n",
    "        print(f\"No hay datos para la clase {name}, saltando...\")\n",
    "        continue  \n",
    "\n",
    "    X_test_processed_cat = X_test[indices_test_cat]\n",
    "    y_test_class1_cat = y_test_class1[indices_test_cat]\n",
    "\n",
    "    print(f\"Clase {name}, Valores únicos en y_test_class1_cat:\", np.unique(y_test_class1_cat, return_counts=True))\n",
    "\n",
    "    # Cargar el modelo correspondiente\n",
    "    model = load_model(os.path.join(\"modelos_cascada_gru\", f\"modelo_{name}.h5\"))\n",
    "\n",
    "    # Hacer la predicción\n",
    "    y_pred_class1 = model.predict(X_test_processed_cat)\n",
    "    y_pred_class1 = np.argmax(y_pred_class1, axis=1)  \n",
    "\n",
    "    accuracy = accuracy_score(y_test_class1_cat, y_pred_class1)\n",
    "    print(f\"Accuracy para {name}: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"*\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6649121,
     "sourceId": 10725550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
