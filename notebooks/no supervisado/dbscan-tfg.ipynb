{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10725550,"sourceType":"datasetVersion","datasetId":6649121}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-17T11:54:37.838907Z","iopub.execute_input":"2025-02-17T11:54:37.839207Z","iopub.status.idle":"2025-02-17T11:54:38.788407Z","shell.execute_reply.started":"2025-02-17T11:54:37.839186Z","shell.execute_reply":"2025-02-17T11:54:38.787625Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/X-IIoTID dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd \ndf = pd.read_csv(\"/kaggle/input/X-IIoTID dataset.csv\", low_memory=False)\n\nrandom_state = 42","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:54:38.789537Z","iopub.execute_input":"2025-02-17T11:54:38.789923Z","iopub.status.idle":"2025-02-17T11:54:57.468888Z","shell.execute_reply.started":"2025-02-17T11:54:38.789887Z","shell.execute_reply":"2025-02-17T11:54:57.468130Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:54:57.470603Z","iopub.execute_input":"2025-02-17T11:54:57.470928Z","iopub.status.idle":"2025-02-17T11:54:59.268176Z","shell.execute_reply.started":"2025-02-17T11:54:57.470899Z","shell.execute_reply":"2025-02-17T11:54:59.267108Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 820834 entries, 0 to 820833\nData columns (total 68 columns):\n #   Column                       Non-Null Count   Dtype \n---  ------                       --------------   ----- \n 0   Date                         820503 non-null  object\n 1   Timestamp                    820537 non-null  object\n 2   Scr_IP                       820834 non-null  object\n 3   Scr_port                     820834 non-null  object\n 4   Des_IP                       820834 non-null  object\n 5   Des_port                     820834 non-null  object\n 6   Protocol                     820834 non-null  object\n 7   Service                      820834 non-null  object\n 8   Duration                     820834 non-null  object\n 9   Scr_bytes                    820834 non-null  object\n 10  Des_bytes                    820834 non-null  object\n 11  Conn_state                   820834 non-null  int64 \n 12  missed_bytes                 820834 non-null  object\n 13  is_syn_only                  820834 non-null  bool  \n 14  Is_SYN_ACK                   820834 non-null  bool  \n 15  is_pure_ack                  820834 non-null  bool  \n 16  is_with_payload              820834 non-null  bool  \n 17  FIN or RST                   820834 non-null  bool  \n 18  Bad_checksum                 820834 non-null  bool  \n 19  is_SYN_with_RST              820834 non-null  bool  \n 20  Scr_pkts                     820834 non-null  object\n 21  Scr_ip_bytes                 820834 non-null  object\n 22  Des_pkts                     820834 non-null  object\n 23  Des_ip_bytes                 820834 non-null  object\n 24  anomaly_alert                820834 non-null  object\n 25  total_bytes                  820834 non-null  object\n 26  total_packet                 820834 non-null  object\n 27  paket_rate                   820834 non-null  object\n 28  byte_rate                    820834 non-null  object\n 29  Scr_packts_ratio             820834 non-null  object\n 30  Des_pkts_ratio               820834 non-null  object\n 31  Scr_bytes_ratio              820834 non-null  object\n 32  Des_bytes_ratio              820834 non-null  object\n 33  Avg_user_time                820834 non-null  object\n 34  Std_user_time                820834 non-null  object\n 35  Avg_nice_time                820834 non-null  object\n 36  Std_nice_time                820834 non-null  object\n 37  Avg_system_time              820834 non-null  object\n 38  Std_system_time              820834 non-null  object\n 39  Avg_iowait_time              820834 non-null  object\n 40  Std_iowait_time              820834 non-null  object\n 41  Avg_ideal_time               820834 non-null  object\n 42  Std_ideal_time               820834 non-null  object\n 43  Avg_tps                      820834 non-null  object\n 44  Std_tps                      820834 non-null  object\n 45  Avg_rtps                     820834 non-null  object\n 46  Std_rtps                     820834 non-null  object\n 47  Avg_wtps                     820834 non-null  object\n 48  Std_wtps                     820834 non-null  object\n 49  Avg_ldavg_1                  820834 non-null  object\n 50  Std_ldavg_1                  820834 non-null  object\n 51  Avg_kbmemused                820834 non-null  object\n 52  Std_kbmemused                820834 non-null  object\n 53  Avg_num_Proc/s               820834 non-null  object\n 54  Std_num_proc/s               820834 non-null  object\n 55  Avg_num_cswch/s              820834 non-null  object\n 56  std_num_cswch/s              820834 non-null  object\n 57  OSSEC_alert                  820834 non-null  int64 \n 58  OSSEC_alert_level            820834 non-null  int64 \n 59  Login_attempt                820834 non-null  int64 \n 60  Succesful_login              820834 non-null  int64 \n 61  File_activity                820834 non-null  int64 \n 62  Process_activity             820834 non-null  int64 \n 63  read_write_physical.process  820834 non-null  int64 \n 64  is_privileged                820834 non-null  int64 \n 65  class1                       820834 non-null  object\n 66  class2                       820834 non-null  object\n 67  class3                       820834 non-null  object\ndtypes: bool(7), int64(9), object(52)\nmemory usage: 387.5+ MB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\"\"\"Divide los datos en entrenamiento, validación y prueba.\"\"\"\nX = df.drop(columns=['class1', 'class2', 'class3'])\ny_class3 = df['class3'].map({'Normal': 0, 'Attack': 1})\ny_class2 = df['class2']\ny_class1 = df['class1']\n\nX_train, X_temp, y_train_class3, y_temp_class3, y_train_class2, y_temp_class2, y_train_class1, y_temp_class1 = train_test_split(\n    X, y_class3, y_class2, y_class1, test_size=0.3, random_state=random_state, stratify=y_class3, shuffle=True\n)\n\nX_val, X_test, y_val_class3, y_test_class3, y_val_class2, y_test_class2, y_val_class1, y_test_class1 = train_test_split(\n    X_temp, y_temp_class3, y_temp_class2, y_temp_class1, test_size=0.5, random_state=random_state, stratify=y_temp_class3, shuffle=True\n)\n\n# Resetear índices para evitar desalineaciones\nX_train = X_train.reset_index(drop=True)\nX_val = X_val.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)  # Opcional\n\ny_train_class3 = y_train_class3.reset_index(drop=True)\ny_val_class3 = y_val_class3.reset_index(drop=True)\ny_test_class3 = y_test_class3.reset_index(drop=True)  # Opcional\n\ny_train_class2 = y_train_class2.reset_index(drop=True)\ny_val_class2 = y_val_class2.reset_index(drop=True)\ny_test_class2 = y_test_class2.reset_index(drop=True)  # Opcional\n\ny_train_class1 = y_train_class1.reset_index(drop=True)\ny_val_class1 = y_val_class1.reset_index(drop=True)\ny_test_class1 = y_test_class1.reset_index(drop=True)  # Opcional","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:54:59.269683Z","iopub.execute_input":"2025-02-17T11:54:59.270006Z","iopub.status.idle":"2025-02-17T11:55:03.323651Z","shell.execute_reply.started":"2025-02-17T11:54:59.269974Z","shell.execute_reply":"2025-02-17T11:55:03.322641Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\ndef fix_dtype(df, umbral_numerico=0.7):\n    object_cols = df.select_dtypes(include=['object']).columns\n    int_cols = df.select_dtypes(include=['int64']).columns\n    bool_cols = df.select_dtypes(include=['bool']).columns\n\n    # Convertir booleanos a float\n    df[bool_cols] = df[bool_cols].astype(float)\n\n    for col in object_cols:\n        valores_unicos = df[col].dropna().unique()\n\n        if {\"true\", \"false\"} <= set(valores_unicos):  # Verifica si ambos existen\n            df[col] = df[col].map({'true': 1, 'false': 0}).astype(float)\n        else:\n            converted = pd.to_numeric(df[col], errors='coerce')\n            if converted.notna().mean() > umbral_numerico:\n                df[col] = converted.astype(float)\n\n    for col in int_cols:\n        df[col] = df[col].astype(float)\n\n    return df\n\ndef delete_ip_port(df):\n    \"\"\"Elimina las columnas 'ip' y 'port'.\"\"\"\n    lista = ['Scr_IP', 'Scr_port', 'Des_IP', 'Des_port', 'Scr_bytes', 'Des_bytes', 'Scr_pkts', \n                            'Des_pkts', 'Scr_ip_bytes', 'Des_ip_bytes', 'Scr_packts_ratio', 'Des_pkts_ratio',\n                            'Scr_bytes_ratio', 'Des_bytes_ratio']\n\n    return df.drop(columns=lista)","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.324703Z","iopub.execute_input":"2025-02-17T11:55:03.325199Z","iopub.status.idle":"2025-02-17T11:55:03.331865Z","shell.execute_reply.started":"2025-02-17T11:55:03.325166Z","shell.execute_reply":"2025-02-17T11:55:03.331026Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\n# Reemplazos comunes de valores\ncommon_replacements = {\n    '-': np.nan,\n    '?': np.nan,\n    'nan': np.nan,\n}\n\ndef replace_common_values(df):\n    \"\"\"Reemplaza valores comunes como '-', '?' y 'nan' por NaN.\"\"\"\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].replace(common_replacements)\n    return df\n\ndef fix_mayus(df):\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].str.lower()\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.332609Z","iopub.execute_input":"2025-02-17T11:55:03.332812Z","iopub.status.idle":"2025-02-17T11:55:03.352806Z","shell.execute_reply.started":"2025-02-17T11:55:03.332795Z","shell.execute_reply":"2025-02-17T11:55:03.352089Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.impute import KNNImputer, SimpleImputer\n\n# Definir los imputadores\nimputers = {\n    'categorical': {\n        'most_frequent': SimpleImputer(strategy='most_frequent'),\n        'knn': KNNImputer(n_neighbors=5)\n    },\n    'numeric': {\n        'mean': SimpleImputer(strategy='mean'),\n        'median': SimpleImputer(strategy='median')\n    }\n}\n\nfrom sklearn.discriminant_analysis import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Definir escaladores\nscalers = {\n    \"robust\": RobustScaler(),\n    \"standard\": StandardScaler()\n}\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\n# Definir codificadores\nencoders = {\n    \"one_hot\": OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n}","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.353572Z","iopub.execute_input":"2025-02-17T11:55:03.353796Z","iopub.status.idle":"2025-02-17T11:55:03.557634Z","shell.execute_reply.started":"2025-02-17T11:55:03.353776Z","shell.execute_reply":"2025-02-17T11:55:03.556751Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"imputador_cat= imputers['categorical']['most_frequent']\nimputador_num = imputers['numeric']['mean']\nnormalizacion = scalers['robust']\ndecodificador = encoders['one_hot']","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.560159Z","iopub.execute_input":"2025-02-17T11:55:03.560387Z","iopub.status.idle":"2025-02-17T11:55:03.563853Z","shell.execute_reply.started":"2025-02-17T11:55:03.560369Z","shell.execute_reply":"2025-02-17T11:55:03.563032Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.decomposition import PCA\nfrom collections import Counter\n\ndef matriz_correlacion(df):\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    correlation_matrix = df[numeric_cols].corr()\n    return correlation_matrix\n\ndef correlacion_pares(df, umbral):\n    df = matriz_correlacion(df)\n    # Toma solo la parte superior de la matriz para evitar duplicados\n    upper_tri = df.where(np.triu(np.ones(df.shape), k=1).astype(bool))\n\n    # Identifica pares altamente correlacionados\n    correlated_pairs = []\n    for col in upper_tri.columns:\n        for row in upper_tri.index:\n            if upper_tri.loc[row, col] > umbral:\n                correlated_pairs.append((row, col))\n\n    # Selecciona las columnas a eliminar (de cada par, se elimina la que aparece como columna)\n    alta_corr_pares = [col for col in upper_tri.columns if any(upper_tri[col] > umbral)]\n    \n    return alta_corr_pares\n\ndef correlacion_respecto_objetivo(df, target, umbral):\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    # Calculamos la correlación con la variable objetivo\n    target_correlation = df[numeric_cols].corrwith(target).abs().sort_values(ascending=True)\n\n    # Nos quedamos solo con las características que tengan correlación >= 0.1\n    baja_corr_respecto_obj = target_correlation[target_correlation < umbral].index.tolist()\n\n    return baja_corr_respecto_obj\n\ndef seleccionar_variables_pca(X_train, X_val, n_components, num_top_features):\n    \"\"\"\n    Aplica PCA para seleccionar las características más influyentes, pero mantiene los datos originales.\n    \n    Parámetros:\n        - X_train: DataFrame de entrenamiento\n        - X_val: DataFrame de validación\n        - n_components: float/int, cantidad de componentes principales o porcentaje de varianza a retener\n        - num_top_features: int, número de características más influyentes a seleccionar\n\n    Retorna:\n        - X_train_filtrado: DataFrame de entrenamiento con las características seleccionadas\n        - X_val_filtrado: DataFrame de validación con las características seleccionadas\n    \"\"\"\n\n    # Si usas el sample, cambialo en las lineas necesarias\n    # X_train_sample = X_train.sample(n=300000, random_state=42) # Seleccionar una muestra de 300,000 instancias como en el articulo\n    \n    # Aplicar PCA (sin guardar la transformación)\n    pca = PCA(n_components=n_components, random_state=42)\n    pca.fit(X_train)  # Solo ajustamos el modelo, no transformamos los datos\n\n    # Obtener nombres originales de las variables\n    original_feature_names = np.array(X_train.columns)\n\n    # Contador de importancia de características en PCA\n    feature_counter = Counter()\n    \n    for comp in pca.components_:\n        top_indices = np.argsort(np.abs(comp))[-num_top_features:]  # Índices de las más importantes\n        top_features = original_feature_names[top_indices]  # Obtener nombres\n        feature_counter.update(top_features)  # Contar ocurrencias\n\n    # Seleccionar las variables más influyentes ordenadas por frecuencia de aparición\n    variables_pca = [feature for feature, _ in feature_counter.most_common()]\n\n    # Filtrar las variables seleccionadas en los conjuntos de datos\n    X_train_filtrado = X_train[variables_pca]\n    X_val_filtrado = X_val[variables_pca]\n    \n    return X_train_filtrado, X_val_filtrado","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.565124Z","iopub.execute_input":"2025-02-17T11:55:03.565432Z","iopub.status.idle":"2025-02-17T11:55:03.581320Z","shell.execute_reply.started":"2025-02-17T11:55:03.565409Z","shell.execute_reply":"2025-02-17T11:55:03.580493Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def calculo_varianza(df):\n    \"\"\"Calcula las varianzas de las columnas de un DataFrame y devuelve las que tienen varianza igual a cero.\"\"\"\n    varianzas = df.var()\n\n    # Identificar columnas con varianza igual a cero\n    variables_con_varianza_cero = [col for col, varianza in varianzas.items() if varianza == 0]\n    \n    return variables_con_varianza_cero\n","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.582227Z","iopub.execute_input":"2025-02-17T11:55:03.582577Z","iopub.status.idle":"2025-02-17T11:55:03.600243Z","shell.execute_reply.started":"2025-02-17T11:55:03.582545Z","shell.execute_reply":"2025-02-17T11:55:03.599531Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train = replace_common_values(X_train)\nX_train = fix_mayus(X_train)\nX_train = fix_dtype(X_train)\nX_train = delete_ip_port(X_train)\n\ny_train_class3 = y_train_class3.loc[X_train.index]\ny_train_class2 = y_train_class2.loc[X_train.index]\ny_train_class1 = y_train_class1.loc[X_train.index]\n\nX_val = replace_common_values(X_val)\nX_val = fix_mayus(X_val)\nX_val = fix_dtype(X_val)\nX_val = delete_ip_port(X_val)\n\ny_val_class3 = y_val_class3.loc[X_val.index]\ny_val_class2 = y_val_class2.loc[X_val.index]\ny_val_class1 = y_val_class1.loc[X_val.index]","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:03.600960Z","iopub.execute_input":"2025-02-17T11:55:03.601222Z","iopub.status.idle":"2025-02-17T11:55:35.881250Z","shell.execute_reply.started":"2025-02-17T11:55:03.601202Z","shell.execute_reply":"2025-02-17T11:55:35.880228Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X_train['Instancia_completa'] = X_train.notnull().all(axis=1).astype(int)\nX_val['Instancia_completa'] = X_val.notnull().all(axis=1).astype(int)\n\ncompletas = X_train['Instancia_completa'].sum()\nincompletas = len(X_train) - completas\nprint(f\"✅ Instancias completas: {completas}, incompletas: {incompletas}\")\nsample_weight_train = X_train['Instancia_completa'].replace({1: 3, 0: 1})\n\ncolumnas_no_comprobar = [col for col in X_train.columns if col not in ['Timestamp', 'Date', 'Instancia_completa'] and X_train[col].dtypes != 'object']\nvariables_con_varianza_cero = calculo_varianza(X_train[columnas_no_comprobar])\nX_train = X_train.drop(columns=variables_con_varianza_cero)\nX_val = X_val.drop(columns=variables_con_varianza_cero)\n    \nX_train = X_train.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\nX_val = X_val.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\n\nalta_corr_pares = correlacion_pares(X_train, 0.97)\nX_train = X_train.drop(columns=alta_corr_pares)\nX_val = X_val.drop(columns=alta_corr_pares)\n\nbaja_corr_respecto_obj = correlacion_respecto_objetivo(X_train, y_train_class3, 0.025)\nX_train = X_train.drop(columns=baja_corr_respecto_obj)\nX_val = X_val.drop(columns=baja_corr_respecto_obj)\n\ncaracteritisticas_seleccionadas = X_train.columns.tolist()\n\nX_train['Protocol'] = X_train['Protocol'].fillna(\"missing\")\nX_val['Protocol'] = X_val['Protocol'].fillna(\"missing\")","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:35.882313Z","iopub.execute_input":"2025-02-17T11:55:35.882680Z","iopub.status.idle":"2025-02-17T11:55:40.717137Z","shell.execute_reply.started":"2025-02-17T11:55:35.882647Z","shell.execute_reply":"2025-02-17T11:55:40.716130Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Instancias completas: 416933, incompletas: 157650\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":" # Identificar columnas categóricas, numéricas y booleanas\ncategorical_cols = X_train.select_dtypes(include=['object']).columns\nboolean_cols = X_train.select_dtypes(include=['bool']).columns\nif boolean_cols.any():  # Si hay columnas booleanas\n    X_train[boolean_cols] = X_train[boolean_cols].astype(float)  # TAL VEZ INNCESESARIO\nnumerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n\n##############################################################################\n    \nX_train[categorical_cols] = imputador_cat.fit_transform(X_train[categorical_cols])\nX_val[categorical_cols] = imputador_cat.transform(X_val[categorical_cols])\n\nX_train[numerical_cols] = imputador_num.fit_transform(X_train[numerical_cols])\nX_val[numerical_cols] = imputador_num.transform(X_val[numerical_cols])\n\n##############################################################################\n\nX_train_scaled = normalizacion.fit_transform(X_train[numerical_cols])\nX_val_scaled = normalizacion.transform(X_val[numerical_cols])\n\n# Convertir las matrices escaladas a DataFrames\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_train.index)\nX_val_scaled_df = pd.DataFrame(X_val_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_val.index)\n\n##############################################################################\n\nX_train_encoded = decodificador.fit_transform(X_train[categorical_cols])\nX_val_encoded = decodificador.transform(X_val[categorical_cols])\n\n# Obtener los nombres de las nuevas columnas codificadas\nencoded_cols = decodificador.get_feature_names_out(categorical_cols)\n\n# Convertir las matrices codificadas a DataFrames\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index)\nX_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_cols, index=X_val.index)\n\n##############################################################################\n\n# Combinar con las características categóricas codificadas\nX_train_processed = pd.concat([X_train_scaled_df, X_train_encoded_df], axis=1)\nX_val_processed = pd.concat([X_val_scaled_df, X_val_encoded_df], axis=1)\n\n# Opcional: Reordenar las columnas si es necesario\nX_train = X_train_processed.reindex(sorted(X_train_processed.columns), axis=1)\nX_val = X_val_processed.reindex(sorted(X_val_processed.columns), axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:40.718151Z","iopub.execute_input":"2025-02-17T11:55:40.718492Z","iopub.status.idle":"2025-02-17T11:55:43.281267Z","shell.execute_reply.started":"2025-02-17T11:55:40.718460Z","shell.execute_reply":"2025-02-17T11:55:43.280562Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"X_train, X_val = seleccionar_variables_pca(X_train_processed, X_val_processed, n_components=0.95, num_top_features=20)\ncaracteritisticas_procesadas = X_train.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2025-02-17T11:55:43.282113Z","iopub.execute_input":"2025-02-17T11:55:43.282355Z","iopub.status.idle":"2025-02-17T11:55:44.515866Z","shell.execute_reply.started":"2025-02-17T11:55:43.282335Z","shell.execute_reply":"2025-02-17T11:55:44.515090Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:55:44.516625Z","iopub.execute_input":"2025-02-17T11:55:44.516943Z","iopub.status.idle":"2025-02-17T11:55:44.523135Z","shell.execute_reply.started":"2025-02-17T11:55:44.516920Z","shell.execute_reply":"2025-02-17T11:55:44.522432Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(574583, 34)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Reducir a 10 dimensiones (ajustable)\npca = PCA(n_components=2)  \nX_reduced = pca.fit_transform(X_train)\n\nprint(f\"Varianza explicada con 2 componentes: {sum(pca.explained_variance_ratio_):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:55:44.523942Z","iopub.execute_input":"2025-02-17T11:55:44.524241Z","iopub.status.idle":"2025-02-17T11:55:46.404796Z","shell.execute_reply.started":"2025-02-17T11:55:44.524211Z","shell.execute_reply":"2025-02-17T11:55:46.403848Z"}},"outputs":[{"name":"stdout","text":"Varianza explicada con 2 componentes: 0.75\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# from sklearn.neighbors import NearestNeighbors\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# neighbors = NearestNeighbors(n_neighbors=35)  # Usar el mismo min_samples\n# neighbors_fit = neighbors.fit(X_train)\n# distances, indices = neighbors_fit.kneighbors(X_train)\n\n# distances = np.sort(distances[:, 34])  # Distancia al 35° vecino más cercano\n# plt.plot(distances)\n# plt.ylabel(\"Distancia al 35° vecino más cercano\")\n# plt.xlabel(\"Puntos ordenados\")\n# plt.title(\"Gráfico para estimar eps en DBSCAN (Datos Escalados)\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:55:46.405694Z","iopub.execute_input":"2025-02-17T11:55:46.406034Z","iopub.status.idle":"2025-02-17T11:55:46.409880Z","shell.execute_reply.started":"2025-02-17T11:55:46.405998Z","shell.execute_reply":"2025-02-17T11:55:46.408941Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\ndb = DBSCAN(eps=0.5, min_samples=10, metric='euclidean')\ndb.fit(X_reduced)\n\nlabels = db.labels_\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint(f\"Número de clusters detectados: {n_clusters_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:55:46.410803Z","iopub.execute_input":"2025-02-17T11:55:46.411109Z","execution_failed":"2025-02-17T11:56:20.837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Graficar los clusters en 2D\nplt.figure(figsize=(10, 6))\nunique_labels = set(labels)\n\nfor label in unique_labels:\n    mask = (labels == label)\n    color = 'black' if label == -1 else plt.cm.jet(label / max(labels + 1))\n    plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1], c=[color], label=f'Cluster {label}', s=10)\n\nplt.legend()\nplt.title(\"Clusters detectados por DBSCAN (PCA 2D)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-17T11:56:20.838Z"}},"outputs":[],"execution_count":null}]}