{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:24.322030Z",
     "iopub.status.busy": "2025-03-04T22:00:24.321735Z",
     "iopub.status.idle": "2025-03-04T22:00:24.334094Z",
     "shell.execute_reply": "2025-03-04T22:00:24.333440Z",
     "shell.execute_reply.started": "2025-03-04T22:00:24.322008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos cargados: 820834 filas, 68 columnas.\n"
     ]
    }
   ],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "path = os.path.join(\"X-IIoTID dataset.csv\")  \n",
    "try:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    print(f\"✅ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: No se encontró el archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:38.991536Z",
     "iopub.status.busy": "2025-03-04T22:00:38.991198Z",
     "iopub.status.idle": "2025-03-04T22:00:40.909252Z",
     "shell.execute_reply": "2025-03-04T22:00:40.908218Z",
     "shell.execute_reply.started": "2025-03-04T22:00:38.991501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820834 entries, 0 to 820833\n",
      "Data columns (total 68 columns):\n",
      " #   Column                       Non-Null Count   Dtype \n",
      "---  ------                       --------------   ----- \n",
      " 0   Date                         820503 non-null  object\n",
      " 1   Timestamp                    820537 non-null  object\n",
      " 2   Scr_IP                       820834 non-null  object\n",
      " 3   Scr_port                     820834 non-null  object\n",
      " 4   Des_IP                       820834 non-null  object\n",
      " 5   Des_port                     820834 non-null  object\n",
      " 6   Protocol                     820834 non-null  object\n",
      " 7   Service                      820834 non-null  object\n",
      " 8   Duration                     820834 non-null  object\n",
      " 9   Scr_bytes                    820834 non-null  object\n",
      " 10  Des_bytes                    820834 non-null  object\n",
      " 11  Conn_state                   820834 non-null  int64 \n",
      " 12  missed_bytes                 820834 non-null  object\n",
      " 13  is_syn_only                  820834 non-null  bool  \n",
      " 14  Is_SYN_ACK                   820834 non-null  bool  \n",
      " 15  is_pure_ack                  820834 non-null  bool  \n",
      " 16  is_with_payload              820834 non-null  bool  \n",
      " 17  FIN or RST                   820834 non-null  bool  \n",
      " 18  Bad_checksum                 820834 non-null  bool  \n",
      " 19  is_SYN_with_RST              820834 non-null  bool  \n",
      " 20  Scr_pkts                     820834 non-null  object\n",
      " 21  Scr_ip_bytes                 820834 non-null  object\n",
      " 22  Des_pkts                     820834 non-null  object\n",
      " 23  Des_ip_bytes                 820834 non-null  object\n",
      " 24  anomaly_alert                820834 non-null  object\n",
      " 25  total_bytes                  820834 non-null  object\n",
      " 26  total_packet                 820834 non-null  object\n",
      " 27  paket_rate                   820834 non-null  object\n",
      " 28  byte_rate                    820834 non-null  object\n",
      " 29  Scr_packts_ratio             820834 non-null  object\n",
      " 30  Des_pkts_ratio               820834 non-null  object\n",
      " 31  Scr_bytes_ratio              820834 non-null  object\n",
      " 32  Des_bytes_ratio              820834 non-null  object\n",
      " 33  Avg_user_time                820834 non-null  object\n",
      " 34  Std_user_time                820834 non-null  object\n",
      " 35  Avg_nice_time                820834 non-null  object\n",
      " 36  Std_nice_time                820834 non-null  object\n",
      " 37  Avg_system_time              820834 non-null  object\n",
      " 38  Std_system_time              820834 non-null  object\n",
      " 39  Avg_iowait_time              820834 non-null  object\n",
      " 40  Std_iowait_time              820834 non-null  object\n",
      " 41  Avg_ideal_time               820834 non-null  object\n",
      " 42  Std_ideal_time               820834 non-null  object\n",
      " 43  Avg_tps                      820834 non-null  object\n",
      " 44  Std_tps                      820834 non-null  object\n",
      " 45  Avg_rtps                     820834 non-null  object\n",
      " 46  Std_rtps                     820834 non-null  object\n",
      " 47  Avg_wtps                     820834 non-null  object\n",
      " 48  Std_wtps                     820834 non-null  object\n",
      " 49  Avg_ldavg_1                  820834 non-null  object\n",
      " 50  Std_ldavg_1                  820834 non-null  object\n",
      " 51  Avg_kbmemused                820834 non-null  object\n",
      " 52  Std_kbmemused                820834 non-null  object\n",
      " 53  Avg_num_Proc/s               820834 non-null  object\n",
      " 54  Std_num_proc/s               820834 non-null  object\n",
      " 55  Avg_num_cswch/s              820834 non-null  object\n",
      " 56  std_num_cswch/s              820834 non-null  object\n",
      " 57  OSSEC_alert                  820834 non-null  int64 \n",
      " 58  OSSEC_alert_level            820834 non-null  int64 \n",
      " 59  Login_attempt                820834 non-null  int64 \n",
      " 60  Succesful_login              820834 non-null  int64 \n",
      " 61  File_activity                820834 non-null  int64 \n",
      " 62  Process_activity             820834 non-null  int64 \n",
      " 63  read_write_physical.process  820834 non-null  int64 \n",
      " 64  is_privileged                820834 non-null  int64 \n",
      " 65  class1                       820834 non-null  object\n",
      " 66  class2                       820834 non-null  object\n",
      " 67  class3                       820834 non-null  object\n",
      "dtypes: bool(7), int64(9), object(52)\n",
      "memory usage: 387.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:22:32.579423Z",
     "iopub.status.busy": "2025-03-04T22:22:32.579044Z",
     "iopub.status.idle": "2025-03-04T22:22:36.233846Z",
     "shell.execute_reply": "2025-03-04T22:22:36.233153Z",
     "shell.execute_reply.started": "2025-03-04T22:22:32.579395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"Divide los datos en entrenamiento, validación y prueba.\"\"\"\n",
    "X = df.drop(columns=['class1', 'class2', 'class3'])\n",
    "y_class3 = df['class3'].map({'Normal': 0, 'Attack': 1})\n",
    "y_class2 = df['class2']\n",
    "y_class1 = df['class1']\n",
    "\n",
    "X_train, X_temp, y_train_class3, y_temp_class3, y_train_class2, y_temp_class2, y_train_class1, y_temp_class1 = train_test_split(\n",
    "    X, y_class3, y_class2, y_class1, test_size=0.3, random_state=random_state, stratify=y_class3, shuffle=True\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val_class3, y_test_class3, y_val_class2, y_test_class2, y_val_class1, y_test_class1 = train_test_split(\n",
    "    X_temp, y_temp_class3, y_temp_class2, y_temp_class1, test_size=0.5, random_state=random_state, stratify=y_temp_class3, shuffle=True\n",
    ")\n",
    "\n",
    "# Resetear índices para evitar desalineaciones\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class3 = y_train_class3.reset_index(drop=True)\n",
    "y_val_class3 = y_val_class3.reset_index(drop=True)\n",
    "y_test_class3 = y_test_class3.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class2 = y_train_class2.reset_index(drop=True)\n",
    "y_val_class2 = y_val_class2.reset_index(drop=True)\n",
    "y_test_class2 = y_test_class2.reset_index(drop=True)  # Opcional\n",
    "\n",
    "y_train_class1 = y_train_class1.reset_index(drop=True)\n",
    "y_val_class1 = y_val_class1.reset_index(drop=True)\n",
    "y_test_class1 = y_test_class1.reset_index(drop=True)  # Opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.871546Z",
     "iopub.status.busy": "2025-03-04T22:00:44.871294Z",
     "iopub.status.idle": "2025-03-04T22:00:44.878195Z",
     "shell.execute_reply": "2025-03-04T22:00:44.877408Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.871524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def fix_dtype(df, umbral_numerico=0.7):\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "\n",
    "    # Convertir booleanos a float\n",
    "    df[bool_cols] = df[bool_cols].astype(float)\n",
    "\n",
    "    for col in object_cols:\n",
    "        valores_unicos = df[col].dropna().unique()\n",
    "\n",
    "        if {\"true\", \"false\"} <= set(valores_unicos):  # Verifica si ambos existen\n",
    "            df[col] = df[col].map({'true': 1, 'false': 0}).astype(float)\n",
    "        else:\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            if converted.notna().mean() > umbral_numerico:\n",
    "                df[col] = converted.astype(float)\n",
    "\n",
    "    for col in int_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def delete_ip_port(df):\n",
    "    \"\"\"Elimina las columnas 'ip' y 'port'.\"\"\"\n",
    "    lista = ['Scr_IP', 'Scr_port', 'Des_IP', 'Des_port', 'Scr_bytes', 'Des_bytes', 'Scr_pkts', \n",
    "                            'Des_pkts', 'Scr_ip_bytes', 'Des_ip_bytes', 'Scr_packts_ratio', 'Des_pkts_ratio',\n",
    "                            'Scr_bytes_ratio', 'Des_bytes_ratio']\n",
    "\n",
    "    return df.drop(columns=lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.879093Z",
     "iopub.status.busy": "2025-03-04T22:00:44.878818Z",
     "iopub.status.idle": "2025-03-04T22:00:44.900540Z",
     "shell.execute_reply": "2025-03-04T22:00:44.899675Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.879065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Reemplazos comunes de valores\n",
    "common_replacements = {\n",
    "    '-': np.nan,\n",
    "    '?': np.nan,\n",
    "    'nan': np.nan,\n",
    "}\n",
    "\n",
    "def replace_common_values(df):\n",
    "    \"\"\"Reemplaza valores comunes como '-', '?' y 'nan' por NaN.\"\"\"\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].replace(common_replacements)\n",
    "    return df\n",
    "\n",
    "def fix_mayus(df):\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.901726Z",
     "iopub.status.busy": "2025-03-04T22:00:44.901415Z",
     "iopub.status.idle": "2025-03-04T22:00:44.917141Z",
     "shell.execute_reply": "2025-03-04T22:00:44.916356Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.901699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Definir los imputadores\n",
    "imputers = {\n",
    "    'categorical': {\n",
    "        'most_frequent': SimpleImputer(strategy='most_frequent'),\n",
    "        'knn': KNNImputer(n_neighbors=5)\n",
    "    },\n",
    "    'numeric': {\n",
    "        'mean': SimpleImputer(strategy='mean'),\n",
    "        'median': SimpleImputer(strategy='median')\n",
    "    }\n",
    "}\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Definir escaladores\n",
    "scalers = {\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"standard\": StandardScaler()\n",
    "}\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Definir codificadores\n",
    "encoders = {\n",
    "    \"one_hot\": OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.918287Z",
     "iopub.status.busy": "2025-03-04T22:00:44.918001Z",
     "iopub.status.idle": "2025-03-04T22:00:44.939138Z",
     "shell.execute_reply": "2025-03-04T22:00:44.938182Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.918258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imputador_cat= imputers['categorical']['most_frequent']\n",
    "imputador_num = imputers['numeric']['mean']\n",
    "normalizacion = scalers['robust']\n",
    "decodificador = encoders['one_hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.942623Z",
     "iopub.status.busy": "2025-03-04T22:00:44.942393Z",
     "iopub.status.idle": "2025-03-04T22:00:44.958255Z",
     "shell.execute_reply": "2025-03-04T22:00:44.957465Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.942596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "def matriz_correlacion(df):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    return correlation_matrix\n",
    "\n",
    "def correlacion_pares(df, umbral):\n",
    "    df = matriz_correlacion(df)\n",
    "    # Toma solo la parte superior de la matriz para evitar duplicados\n",
    "    upper_tri = df.where(np.triu(np.ones(df.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identifica pares altamente correlacionados\n",
    "    correlated_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        for row in upper_tri.index:\n",
    "            if upper_tri.loc[row, col] > umbral:\n",
    "                correlated_pairs.append((row, col))\n",
    "\n",
    "    # Selecciona las columnas a eliminar (de cada par, se elimina la que aparece como columna)\n",
    "    alta_corr_pares = [col for col in upper_tri.columns if any(upper_tri[col] > umbral)]\n",
    "    \n",
    "    return alta_corr_pares\n",
    "\n",
    "def correlacion_respecto_objetivo(df, target, umbral):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Calculamos la correlación con la variable objetivo\n",
    "    target_correlation = df[numeric_cols].corrwith(target).abs().sort_values(ascending=True)\n",
    "\n",
    "    # Nos quedamos solo con las características que tengan correlación >= 0.1\n",
    "    baja_corr_respecto_obj = target_correlation[target_correlation < umbral].index.tolist()\n",
    "\n",
    "    return baja_corr_respecto_obj\n",
    "\n",
    "def seleccionar_variables_pca(X_train, X_val, n_components, num_top_features):\n",
    "    \"\"\"\n",
    "    Aplica PCA para seleccionar las características más influyentes, pero mantiene los datos originales.\n",
    "    \n",
    "    Parámetros:\n",
    "        - X_train: DataFrame de entrenamiento\n",
    "        - X_val: DataFrame de validación\n",
    "        - n_components: float/int, cantidad de componentes principales o porcentaje de varianza a retener\n",
    "        - num_top_features: int, número de características más influyentes a seleccionar\n",
    "\n",
    "    Retorna:\n",
    "        - X_train_filtrado: DataFrame de entrenamiento con las características seleccionadas\n",
    "        - X_val_filtrado: DataFrame de validación con las características seleccionadas\n",
    "    \"\"\"\n",
    "\n",
    "    # Si usas el sample, cambialo en las lineas necesarias\n",
    "    # X_train_sample = X_train.sample(n=300000, random_state=42) # Seleccionar una muestra de 300,000 instancias como en el articulo\n",
    "    \n",
    "    # Aplicar PCA (sin guardar la transformación)\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    pca.fit(X_train)  # Solo ajustamos el modelo, no transformamos los datos\n",
    "\n",
    "    # Obtener nombres originales de las variables\n",
    "    original_feature_names = np.array(X_train.columns)\n",
    "\n",
    "    # Contador de importancia de características en PCA\n",
    "    feature_counter = Counter()\n",
    "    \n",
    "    for comp in pca.components_:\n",
    "        top_indices = np.argsort(np.abs(comp))[-num_top_features:]  # Índices de las más importantes\n",
    "        top_features = original_feature_names[top_indices]  # Obtener nombres\n",
    "        feature_counter.update(top_features)  # Contar ocurrencias\n",
    "\n",
    "    # Seleccionar las variables más influyentes ordenadas por frecuencia de aparición\n",
    "    variables_pca = [feature for feature, _ in feature_counter.most_common()]\n",
    "\n",
    "    # Filtrar las variables seleccionadas en los conjuntos de datos\n",
    "    X_train_filtrado = X_train[variables_pca]\n",
    "    X_val_filtrado = X_val[variables_pca]\n",
    "    \n",
    "    return X_train_filtrado, X_val_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.960390Z",
     "iopub.status.busy": "2025-03-04T22:00:44.960153Z",
     "iopub.status.idle": "2025-03-04T22:00:44.982406Z",
     "shell.execute_reply": "2025-03-04T22:00:44.981553Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.960371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculo_varianza(df):\n",
    "    \"\"\"Calcula las varianzas de las columnas de un DataFrame y devuelve las que tienen varianza igual a cero.\"\"\"\n",
    "    varianzas = df.var()\n",
    "\n",
    "    # Identificar columnas con varianza igual a cero\n",
    "    variables_con_varianza_cero = [col for col, varianza in varianzas.items() if varianza == 0]\n",
    "    \n",
    "    return variables_con_varianza_cero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:00:44.983733Z",
     "iopub.status.busy": "2025-03-04T22:00:44.983404Z",
     "iopub.status.idle": "2025-03-04T22:01:16.561816Z",
     "shell.execute_reply": "2025-03-04T22:01:16.561075Z",
     "shell.execute_reply.started": "2025-03-04T22:00:44.983704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = replace_common_values(X_train)\n",
    "X_train = fix_mayus(X_train)\n",
    "X_train = fix_dtype(X_train)\n",
    "X_train = delete_ip_port(X_train)\n",
    "\n",
    "y_train_class3 = y_train_class3.loc[X_train.index]\n",
    "y_train_class2 = y_train_class2.loc[X_train.index]\n",
    "y_train_class1 = y_train_class1.loc[X_train.index]\n",
    "\n",
    "X_val = replace_common_values(X_val)\n",
    "X_val = fix_mayus(X_val)\n",
    "X_val = fix_dtype(X_val)\n",
    "X_val = delete_ip_port(X_val)\n",
    "\n",
    "y_val_class3 = y_val_class3.loc[X_val.index]\n",
    "y_val_class2 = y_val_class2.loc[X_val.index]\n",
    "y_val_class1 = y_val_class1.loc[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:20:27.080514Z",
     "iopub.status.busy": "2025-03-04T22:20:27.080077Z",
     "iopub.status.idle": "2025-03-04T22:20:31.272883Z",
     "shell.execute_reply": "2025-03-04T22:20:31.271899Z",
     "shell.execute_reply.started": "2025-03-04T22:20:27.080476Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instancias completas: 416933, incompletas: 157650\n"
     ]
    }
   ],
   "source": [
    "X_train['Instancia_completa'] = X_train.notnull().all(axis=1).astype(int)\n",
    "X_val['Instancia_completa'] = X_val.notnull().all(axis=1).astype(int)\n",
    "\n",
    "completas = X_train['Instancia_completa'].sum()\n",
    "incompletas = len(X_train) - completas\n",
    "print(f\"✅ Instancias completas: {completas}, incompletas: {incompletas}\")\n",
    "sample_weight_train = X_train['Instancia_completa'].replace({1: 3, 0: 1})\n",
    "\n",
    "columnas_no_comprobar = [col for col in X_train.columns if col not in ['Timestamp', 'Date', 'Instancia_completa'] and X_train[col].dtypes != 'object']\n",
    "variables_con_varianza_cero = calculo_varianza(X_train[columnas_no_comprobar])\n",
    "X_train = X_train.drop(columns=variables_con_varianza_cero)\n",
    "X_val = X_val.drop(columns=variables_con_varianza_cero)\n",
    "    \n",
    "X_train = X_train.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\n",
    "X_val = X_val.drop(columns=['Timestamp', 'Date', 'Instancia_completa'], errors='ignore')\n",
    "\n",
    "alta_corr_pares = correlacion_pares(X_train, 0.97)\n",
    "X_train = X_train.drop(columns=alta_corr_pares)\n",
    "X_val = X_val.drop(columns=alta_corr_pares)\n",
    "\n",
    "baja_corr_respecto_obj = correlacion_respecto_objetivo(X_train, y_train_class3, 0.025)\n",
    "X_train = X_train.drop(columns=baja_corr_respecto_obj)\n",
    "X_val = X_val.drop(columns=baja_corr_respecto_obj)\n",
    "\n",
    "caracteritisticas_seleccionadas = X_train.columns.tolist()\n",
    "\n",
    "X_train['Protocol'] = X_train['Protocol'].fillna(\"missing\")\n",
    "X_val['Protocol'] = X_val['Protocol'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:01:21.406854Z",
     "iopub.status.busy": "2025-03-04T22:01:21.406638Z",
     "iopub.status.idle": "2025-03-04T22:01:24.205399Z",
     "shell.execute_reply": "2025-03-04T22:01:24.204668Z",
     "shell.execute_reply.started": "2025-03-04T22:01:21.406835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Identificar columnas categóricas, numéricas y booleanas\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "boolean_cols = X_train.select_dtypes(include=['bool']).columns\n",
    "if boolean_cols.any():  # Si hay columnas booleanas\n",
    "    X_train[boolean_cols] = X_train[boolean_cols].astype(float)  # TAL VEZ INNCESESARIO\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "##############################################################################\n",
    "    \n",
    "X_train[categorical_cols] = imputador_cat.fit_transform(X_train[categorical_cols])\n",
    "X_val[categorical_cols] = imputador_cat.transform(X_val[categorical_cols])\n",
    "\n",
    "X_train[numerical_cols] = imputador_num.fit_transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = imputador_num.transform(X_val[numerical_cols])\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train_scaled = normalizacion.fit_transform(X_train[numerical_cols])\n",
    "X_val_scaled = normalizacion.transform(X_val[numerical_cols])\n",
    "\n",
    "# Convertir las matrices escaladas a DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_train.index)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_val.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train_encoded = decodificador.fit_transform(X_train[categorical_cols])\n",
    "X_val_encoded = decodificador.transform(X_val[categorical_cols])\n",
    "\n",
    "# Obtener los nombres de las nuevas columnas codificadas\n",
    "encoded_cols = decodificador.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Convertir las matrices codificadas a DataFrames\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index)\n",
    "X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_cols, index=X_val.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Combinar con las características categóricas codificadas\n",
    "X_train_processed = pd.concat([X_train_scaled_df, X_train_encoded_df], axis=1)\n",
    "X_val_processed = pd.concat([X_val_scaled_df, X_val_encoded_df], axis=1)\n",
    "\n",
    "# Opcional: Reordenar las columnas si es necesario\n",
    "X_train = X_train_processed.reindex(sorted(X_train_processed.columns), axis=1)\n",
    "X_val = X_val_processed.reindex(sorted(X_val_processed.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:01:24.206527Z",
     "iopub.status.busy": "2025-03-04T22:01:24.206304Z",
     "iopub.status.idle": "2025-03-04T22:01:25.341273Z",
     "shell.execute_reply": "2025-03-04T22:01:25.340581Z",
     "shell.execute_reply.started": "2025-03-04T22:01:24.206506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val = seleccionar_variables_pca(X_train_processed, X_val_processed, n_components=0.95, num_top_features=20)\n",
    "caracteritisticas_procesadas = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:01:25.342409Z",
     "iopub.status.busy": "2025-03-04T22:01:25.342152Z",
     "iopub.status.idle": "2025-03-04T22:01:25.422582Z",
     "shell.execute_reply": "2025-03-04T22:01:25.421765Z",
     "shell.execute_reply.started": "2025-03-04T22:01:25.342387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574583, 34) (123125, 34)\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)\n",
    "print(X_train.isnull().sum().sum(), X_val.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder_class2 = LabelEncoder()\n",
    "y_train_class2 = label_encoder_class2.fit_transform(y_train_class2)\n",
    "y_val_class2   = label_encoder_class2.transform(y_val_class2)\n",
    "y_test_class2  = label_encoder_class2.transform(y_test_class2)\n",
    "\n",
    "mapping_class2 = dict(enumerate(label_encoder_class2.classes_))\n",
    "\n",
    "label_encoder_class1 = LabelEncoder()\n",
    "y_train_class1 = label_encoder_class1.fit_transform(y_train_class1)\n",
    "y_val_class1   = label_encoder_class1.transform(y_val_class1)\n",
    "y_test_class1  = label_encoder_class1.transform(y_test_class1)\n",
    "\n",
    "mapping_class1 = dict(enumerate(label_encoder_class1.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'C&C', 1: 'Exfiltration', 2: 'Exploitation', 3: 'Lateral _movement', 4: 'Normal', 5: 'RDOS', 6: 'Reconnaissance', 7: 'Tampering', 8: 'Weaponization', 9: 'crypto-ransomware'}\n",
      "{0: 'BruteForce', 1: 'C&C', 2: 'Dictionary', 3: 'Discovering_resources', 4: 'Exfiltration', 5: 'Fake_notification', 6: 'False_data_injection', 7: 'Generic_scanning', 8: 'MQTT_cloud_broker_subscription', 9: 'MitM', 10: 'Modbus_register_reading', 11: 'Normal', 12: 'RDOS', 13: 'Reverse_shell', 14: 'Scanning_vulnerability', 15: 'TCP Relay', 16: 'crypto-ransomware', 17: 'fuzzing', 18: 'insider_malcious'}\n"
     ]
    }
   ],
   "source": [
    "print(mapping_class2)\n",
    "print(mapping_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:01:25.423503Z",
     "iopub.status.busy": "2025-03-04T22:01:25.423293Z",
     "iopub.status.idle": "2025-03-04T22:01:25.501060Z",
     "shell.execute_reply": "2025-03-04T22:01:25.500214Z",
     "shell.execute_reply.started": "2025-03-04T22:01:25.423484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Número de características de entrada\n",
    "num_classes_2 = len(set(y_train_class2))  # Cantidad de clases en y_class2\n",
    "num_classes_1 = len(set(y_train_class1))  # Cantidad de clases en y_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:02:09.971353Z",
     "iopub.status.busy": "2025-03-04T22:02:09.971017Z",
     "iopub.status.idle": "2025-03-04T22:02:10.032147Z",
     "shell.execute_reply": "2025-03-04T22:02:10.031314Z",
     "shell.execute_reply.started": "2025-03-04T22:02:09.971320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_class2 = y_train_class2.ravel()\n",
    "y_val_class2 = y_val_class2.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:02:10.197896Z",
     "iopub.status.busy": "2025-03-04T22:02:10.197595Z",
     "iopub.status.idle": "2025-03-04T22:02:30.946610Z",
     "shell.execute_reply": "2025-03-04T22:02:30.945713Z",
     "shell.execute_reply.started": "2025-03-04T22:02:10.197866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9492 - loss: 0.1957 - val_accuracy: 0.9826 - val_loss: 0.0652\n",
      "Epoch 2/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9828 - loss: 0.0551 - val_accuracy: 0.9821 - val_loss: 0.0620\n",
      "Epoch 3/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.9846 - loss: 0.0471 - val_accuracy: 0.9856 - val_loss: 0.0456\n",
      "Epoch 4/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0422 - val_accuracy: 0.9859 - val_loss: 0.0411\n",
      "Epoch 5/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9868 - loss: 0.0389 - val_accuracy: 0.9878 - val_loss: 0.0374\n",
      "Epoch 6/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.0358 - val_accuracy: 0.9875 - val_loss: 0.0370\n",
      "Epoch 7/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9889 - loss: 0.0335 - val_accuracy: 0.9888 - val_loss: 0.0373\n",
      "Epoch 8/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0326 - val_accuracy: 0.9888 - val_loss: 0.0341\n",
      "Epoch 9/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9898 - loss: 0.0315 - val_accuracy: 0.9893 - val_loss: 0.0324\n",
      "Epoch 10/10\n",
      "\u001b[1m2299/2299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9903 - loss: 0.0293 - val_accuracy: 0.9893 - val_loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\dnn_class2.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Entrada\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Capas ocultas\n",
    "x = layers.Dense(200, activation='relu')(input_layer)\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "\n",
    "# Salidas\n",
    "output_class2 = layers.Dense(num_classes_2, activation='softmax', name=\"output_class2\")(x)  # Multiclase Categoria\n",
    "\n",
    "# Modelo con dos salidas\n",
    "model_class2 = keras.Model(inputs=input_layer, outputs=output_class2)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_class2.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir el callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_class2.fit(X_train,\n",
    "                    y_train_class2,\n",
    "                    batch_size=250,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val_class2),\n",
    "                   callbacks=[early_stopping])\n",
    "\n",
    "# Guardar el modelo con un nombre único\n",
    "model_name = f\"dnn_class2\"\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "carpeta_modelos = \"modelos_dnn_opcion3\"\n",
    "os.makedirs(carpeta_modelos, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo dentro de la carpeta\n",
    "ruta_modelo = os.path.join(carpeta_modelos, f\"{model_name}.h5\")\n",
    "model_class2.save(ruta_modelo)\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_modelo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:02:30.948025Z",
     "iopub.status.busy": "2025-03-04T22:02:30.947714Z",
     "iopub.status.idle": "2025-03-04T22:02:33.920231Z",
     "shell.execute_reply": "2025-03-04T22:02:33.919367Z",
     "shell.execute_reply.started": "2025-03-04T22:02:30.947995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3848/3848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "(123125,)\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase2\n",
    "y_pred_class2 = model_class2.predict(X_val)\n",
    "y_pred_class2 = np.argmax(y_pred_class2, axis=1)\n",
    "print(y_pred_class2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name_to_number = {value: key for key, value in mapping_class2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279592,)\n",
      "(279592,)\n"
     ]
    }
   ],
   "source": [
    "indices_train = np.where(y_train_class3 == 1)[0]\n",
    "X_train1 = X_train.iloc[indices_train]\n",
    "y_train_class11 = y_train_class1[indices_train].ravel()\n",
    "y_train_class21 = y_train_class2[indices_train].ravel()\n",
    "print(indices_train.shape)\n",
    "indices_train = np.where(y_train_class2 != class_name_to_number['Normal'])[0]\n",
    "X_train1 = X_train.iloc[indices_train]\n",
    "y_train_class11 = y_train_class1[indices_train].ravel()\n",
    "y_train_class21 = y_train_class2[indices_train].ravel()\n",
    "print(indices_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:02:33.921425Z",
     "iopub.status.busy": "2025-03-04T22:02:33.921089Z",
     "iopub.status.idle": "2025-03-04T22:02:33.950982Z",
     "shell.execute_reply": "2025-03-04T22:02:33.950095Z",
     "shell.execute_reply.started": "2025-03-04T22:02:33.921401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "indices_train = np.where(y_train_class2 != class_name_to_number['Normal'])[0]\n",
    "X_train = X_train.iloc[indices_train]\n",
    "y_train_class1 = y_train_class1[indices_train].ravel()\n",
    "y_train_class2 = y_train_class2[indices_train].ravel()\n",
    "\n",
    "indices_val = np.where(y_pred_class2 != class_name_to_number['Normal'])[0]\n",
    "X_val = X_val.iloc[indices_val]\n",
    "y_val_class1 = y_val_class1[indices_val].ravel()\n",
    "y_val_class2 = y_val_class2[indices_val].ravel()\n",
    "\n",
    "y_pred_class2 = y_pred_class2[indices_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:02:34.061773Z",
     "iopub.status.busy": "2025-03-04T22:02:34.061491Z",
     "iopub.status.idle": "2025-03-04T22:02:51.356674Z",
     "shell.execute_reply": "2025-03-04T22:02:51.355961Z",
     "shell.execute_reply.started": "2025-03-04T22:02:34.061751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6976 - loss: 1.2355 - val_accuracy: 0.9580 - val_loss: 0.3777\n",
      "Epoch 2/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9694 - loss: 0.1286 - val_accuracy: 0.9720 - val_loss: 0.4154\n",
      "Epoch 3/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0364 - val_accuracy: 0.9720 - val_loss: 0.4717\n",
      "Epoch 4/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9983 - loss: 0.0162 - val_accuracy: 0.9720 - val_loss: 0.5186\n",
      "Epoch 5/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9790 - val_loss: 0.5204\n",
      "Epoch 6/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9790 - val_loss: 0.5458\n",
      "Epoch 7/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9790 - val_loss: 0.5671\n",
      "Epoch 8/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.2407e-04 - val_accuracy: 0.9790 - val_loss: 0.5787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\Exploitation_class1.h5\n",
      "**************************************************\n",
      "Epoch 1/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9508 - loss: 0.3268 - val_accuracy: 0.9889 - val_loss: 0.1869\n",
      "Epoch 2/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9966 - loss: 0.0714 - val_accuracy: 0.9884 - val_loss: 0.1927\n",
      "Epoch 3/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0250 - val_accuracy: 0.9893 - val_loss: 0.1810\n",
      "Epoch 4/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0222 - val_accuracy: 0.9891 - val_loss: 0.1964\n",
      "Epoch 5/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9988 - loss: 0.0264 - val_accuracy: 0.9889 - val_loss: 0.2000\n",
      "Epoch 6/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0082 - val_accuracy: 0.9893 - val_loss: 0.2098\n",
      "Epoch 7/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0220 - val_accuracy: 0.9887 - val_loss: 0.2044\n",
      "Epoch 8/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0103 - val_accuracy: 0.9891 - val_loss: 0.2006\n",
      "Epoch 9/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0044 - val_accuracy: 0.9893 - val_loss: 0.2110\n",
      "Epoch 10/15\n",
      "\u001b[1m345/345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0161 - val_accuracy: 0.9893 - val_loss: 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\Lateral _movement_class1.h5\n",
      "**************************************************\n",
      "Epoch 1/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1386 - val_accuracy: 0.9876 - val_loss: 0.2998\n",
      "Epoch 2/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0054 - val_accuracy: 0.9875 - val_loss: 0.3241\n",
      "Epoch 3/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0065 - val_accuracy: 0.9876 - val_loss: 0.3084\n",
      "Epoch 4/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0027 - val_accuracy: 0.9876 - val_loss: 0.3228\n",
      "Epoch 5/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 4.4699e-04 - val_accuracy: 0.9876 - val_loss: 0.3234\n",
      "Epoch 6/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 1.6482e-04 - val_accuracy: 0.9876 - val_loss: 0.3315\n",
      "Epoch 7/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.9882e-05 - val_accuracy: 0.9876 - val_loss: 0.3499\n",
      "Epoch 8/15\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.7761e-06 - val_accuracy: 0.9876 - val_loss: 0.3459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\Reconnaissance_class1.h5\n",
      "**************************************************\n",
      "Epoch 1/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9151 - loss: 0.5013 - val_accuracy: 0.9712 - val_loss: 14.5006\n",
      "Epoch 2/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9738 - val_loss: 14.9617\n",
      "Epoch 3/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.7814e-04 - val_accuracy: 0.9738 - val_loss: 16.8154\n",
      "Epoch 4/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.4156e-05 - val_accuracy: 0.9738 - val_loss: 17.1003\n",
      "Epoch 5/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.5590e-05 - val_accuracy: 0.9738 - val_loss: 17.2975\n",
      "Epoch 6/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.9269e-05 - val_accuracy: 0.9738 - val_loss: 17.4648\n",
      "Epoch 7/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.1481e-05 - val_accuracy: 0.9738 - val_loss: 17.5326\n",
      "Epoch 8/15\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.3328e-05 - val_accuracy: 0.9738 - val_loss: 18.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\Tampering_class1.h5\n",
      "**************************************************\n",
      "Epoch 1/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9796 - loss: 0.0808 - val_accuracy: 0.9999 - val_loss: 0.0025\n",
      "Epoch 2/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.2807e-06 - val_accuracy: 0.9999 - val_loss: 0.0027\n",
      "Epoch 3/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.1221e-06 - val_accuracy: 0.9999 - val_loss: 0.0027\n",
      "Epoch 4/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.6241e-06 - val_accuracy: 0.9999 - val_loss: 0.0028\n",
      "Epoch 5/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0596e-06 - val_accuracy: 0.9999 - val_loss: 0.0028\n",
      "Epoch 6/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4546e-06 - val_accuracy: 0.9999 - val_loss: 0.0028\n",
      "Epoch 7/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0672e-06 - val_accuracy: 0.9999 - val_loss: 0.0029\n",
      "Epoch 8/15\n",
      "\u001b[1m736/736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.2231e-06 - val_accuracy: 0.9999 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: modelos_dnn_opcion3\\Weaponization_class1.h5\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "modelos = []  # Lista para almacenar los modelos\n",
    "\n",
    "class_names = np.unique(y_train_class2)\n",
    "for name in class_names:\n",
    "\n",
    "    indices_train_cat = np.where(y_train_class2 == name)[0]\n",
    "    X_train_cat = X_train.iloc[indices_train_cat]\n",
    "    y_train_class1_cat = y_train_class1[indices_train_cat]\n",
    "    \n",
    "    if np.unique(y_train_class1_cat).size == 1:\n",
    "        continue  \n",
    "\n",
    "    indices_val_cat = np.where(y_pred_class2 == name)[0]\n",
    "    X_val_cat = X_val.iloc[indices_val_cat]\n",
    "    y_val_class1_cat = y_val_class1[indices_val_cat]\n",
    "\n",
    "    # Entrada\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Capas ocultas\n",
    "    x = layers.Dense(200, activation='relu')(input_layer)\n",
    "    x = layers.Dense(200, activation='relu')(x)\n",
    "    x = layers.Dense(200, activation='relu')(x)\n",
    "    \n",
    "    # Salidas\n",
    "    output_class1 = layers.Dense(num_classes_1, activation='softmax', name=\"output_class1\")(x)  # Multiclase Categoria\n",
    "    \n",
    "    # Modelo con dos salidas\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_class1)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Definir el callback EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(X_train_cat,\n",
    "                        y_train_class1_cat,\n",
    "                        batch_size=64,\n",
    "                        epochs=15,\n",
    "                        validation_data=(X_val_cat, y_val_class1_cat),\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Guardar el modelo con un nombre único\n",
    "    nombre = mapping_class2[name]\n",
    "    model_name = f\"{nombre}_class1\"\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    carpeta_modelos = \"modelos_dnn_opcion3\"\n",
    "    os.makedirs(carpeta_modelos, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo dentro de la carpeta\n",
    "    ruta_modelo = os.path.join(carpeta_modelos, f\"{model_name}.h5\")\n",
    "    model.save(ruta_modelo)\n",
    "\n",
    "    print(f\"Modelo guardado en: {ruta_modelo}\")\n",
    "\n",
    "    \n",
    "    # Añadir el modelo a la lista\n",
    "    modelos.append((model_name, model))  # Almacena el nombre y el modelo\n",
    "    \n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:23:04.513589Z",
     "iopub.status.busy": "2025-03-04T22:23:04.513289Z",
     "iopub.status.idle": "2025-03-04T22:23:09.940421Z",
     "shell.execute_reply": "2025-03-04T22:23:09.939141Z",
     "shell.execute_reply.started": "2025-03-04T22:23:04.513567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test = replace_common_values(X_test)\n",
    "X_test = fix_mayus(X_test)\n",
    "X_test = fix_dtype(X_test)\n",
    "X_test = delete_ip_port(X_test)\n",
    "\n",
    "y_test_class3 = y_test_class3[X_test.index]\n",
    "\n",
    "X_test = X_test[caracteritisticas_seleccionadas] \n",
    "\n",
    "X_test['Protocol'] = X_test['Protocol'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:22:19.991724Z",
     "iopub.status.busy": "2025-03-04T22:22:19.991425Z",
     "iopub.status.idle": "2025-03-04T22:22:20.078513Z",
     "shell.execute_reply": "2025-03-04T22:22:20.077337Z",
     "shell.execute_reply.started": "2025-03-04T22:22:19.991701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas, numéricas y booleanas\n",
    "categorical_cols = X_test.select_dtypes(include=['object']).columns\n",
    "boolean_cols = X_test.select_dtypes(include=['bool']).columns\n",
    "if boolean_cols.any():  # Si hay columnas booleanas\n",
    "    X_test[boolean_cols] = X_test[boolean_cols].astype(float) # TAL VEZ INNCESESARIO\n",
    "numerical_cols = X_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test[categorical_cols] = imputador_cat.transform(X_test[categorical_cols])\n",
    "\n",
    "X_test[numerical_cols] = imputador_num.transform(X_test[numerical_cols])\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test_scaled = normalizacion.transform(X_test[numerical_cols])\n",
    "\n",
    "# Convertir las matrices escaladas a DataFrames\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f\"{col}_scaled\" for col in numerical_cols], index=X_test.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_test_encoded = decodificador.transform(X_test[categorical_cols])\n",
    "\n",
    "# Obtener los nombres de las nuevas columnas codificadas\n",
    "encoded_cols = decodificador.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Convertir las matrices codificadas a DataFrames\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_cols, index=X_test.index)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Combinar con las características categóricas codificadas\n",
    "X_test_processed = pd.concat([X_test_scaled_df, X_test_encoded_df], axis=1)\n",
    "\n",
    "# Opcional: Reordenar las columnas si es necesario\n",
    "X_test_processed = X_test_processed.reindex(sorted(X_test_processed.columns), axis=1)\n",
    "\n",
    "X_test = X_test_processed[caracteritisticas_procesadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:08:12.944642Z",
     "iopub.status.busy": "2025-03-04T22:08:12.944336Z",
     "iopub.status.idle": "2025-03-04T22:08:12.976905Z",
     "shell.execute_reply": "2025-03-04T22:08:12.976180Z",
     "shell.execute_reply.started": "2025-03-04T22:08:12.944605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_class2 = y_test_class2.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:08:26.930934Z",
     "iopub.status.busy": "2025-03-04T22:08:26.930651Z",
     "iopub.status.idle": "2025-03-04T22:08:29.612913Z",
     "shell.execute_reply": "2025-03-04T22:08:29.611934Z",
     "shell.execute_reply.started": "2025-03-04T22:08:26.930913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3848/3848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 994us/step\n"
     ]
    }
   ],
   "source": [
    "# Predicción para salida de clase2\n",
    "y_pred_class2 = model_class2.predict(X_test)\n",
    "y_pred_class2 = np.argmax(y_pred_class2, axis=1)\n",
    "y_pred_class2_names = label_encoder_class2.inverse_transform(y_pred_class2)\n",
    "predicciones_df = pd.DataFrame({'Predicciones_Class2': y_pred_class2_names})    \n",
    "ruta_directorio = '../../predicciones'\n",
    "os.makedirs(ruta_directorio, exist_ok=True)\n",
    "predicciones_df.to_csv(os.path.join(ruta_directorio, 'arq3.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:08:29.875022Z",
     "iopub.status.busy": "2025-03-04T22:08:29.874757Z",
     "iopub.status.idle": "2025-03-04T22:08:29.881673Z",
     "shell.execute_reply": "2025-03-04T22:08:29.880911Z",
     "shell.execute_reply.started": "2025-03-04T22:08:29.874997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Accuracy (Test): 0.9893\n",
      "📈 Precision (Test): 0.9807\n",
      "📈 Recall (Test): 0.9458\n",
      "📈 F1 (Test): 0.9618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_class2, y_pred_class2)\n",
    "print(f'📈 Accuracy (Test): {accuracy:.4f}')\n",
    "precision = precision_score(y_test_class2, y_pred_class2, average='macro')\n",
    "print(f'📈 Precision (Test): {precision:.4f}')\n",
    "recall = recall_score(y_test_class2, y_pred_class2, average='macro')\n",
    "print(f'📈 Recall (Test): {recall:.4f}')\n",
    "f1 = f1_score(y_test_class2, y_pred_class2, average='macro')\n",
    "print(f'📈 F1 (Test): {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase2 Mapping: {0: 'C&C', 1: 'Exfiltration', 2: 'Exploitation', 3: 'Lateral _movement', 4: 'RDOS', 5: 'Reconnaissance', 6: 'Tampering', 7: 'Weaponization', 8: 'crypto-ransomware'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase1 Mapping: {0: 'BruteForce', 1: 'C&C', 2: 'Dictionary', 3: 'Discovering_resources', 4: 'Exfiltration', 5: 'Fake_notification', 6: 'False_data_injection', 7: 'Generic_scanning', 8: 'MQTT_cloud_broker_subscription', 9: 'MitM', 10: 'Modbus_register_reading', 11: 'RDOS', 12: 'Reverse_shell', 13: 'Scanning_vulnerability', 14: 'TCP Relay', 15: 'crypto-ransomware', 16: 'fuzzing', 17: 'insider_malcious'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_anomalia_test = np.where(y_pred_class2_names != \"Normal\")[0]\n",
    "X_test_processed = X_test_processed.iloc[indices_anomalia_test]\n",
    "y_pred_class2= y_pred_class2[indices_anomalia_test]\n",
    "y_test_class1 = y_test_class1[indices_anomalia_test].ravel()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:13:53.626487Z",
     "iopub.status.busy": "2025-03-04T22:13:53.626091Z",
     "iopub.status.idle": "2025-03-04T22:13:55.844187Z",
     "shell.execute_reply": "2025-03-04T22:13:55.843325Z",
     "shell.execute_reply.started": "2025-03-04T22:13:53.626457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploitation\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lateral _movement\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnaissance\n",
      "\u001b[1m584/584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Tampering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Weaponization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "y_pred_class1_total = [\"Normal\"] * len(indices_anomalia_test)  # Inicializa con \"Normal\"\n",
    "ruta_directorio = '../../predicciones'\n",
    "predicciones_df = pd.read_csv(os.path.join(ruta_directorio, 'arq3.csv'))\n",
    "    \n",
    "class_names = ['Exploitation', 'Lateral _movement', 'Reconnaissance', 'Tampering', 'Weaponization']\n",
    "\n",
    "for name in class_names:\n",
    "    \n",
    "    print(name)\n",
    "\n",
    "    class_number = class_name_to_number[name]\n",
    "    # Filtrar índices de la clase\n",
    "    indices_test_cat = np.where(y_pred_class2 == class_number)[0]\n",
    "\n",
    "    X_test_processed_cat = X_test.iloc[indices_test_cat]\n",
    "    y_test_class1_cat = y_test_class1[indices_test_cat]\n",
    "\n",
    "    # Cargar el modelo correspondiente\n",
    "    model = load_model(os.path.join(\"modelos_dnn_opcion3\", f\"{name}_class1.h5\"))\n",
    "\n",
    "    # Hacer la predicción\n",
    "    y_pred_class1 = model.predict(X_test_processed_cat)\n",
    "    y_pred_class1 = np.argmax(y_pred_class1, axis=1)  \n",
    "    \n",
    "    y_pred_class1_names = label_encoder_class1.inverse_transform(y_pred_class1)\n",
    "\n",
    "    # Asignar las predicciones a la lista total usando el índice correcto\n",
    "    for idx, pred in zip(indices_test_cat, y_pred_class1_names):\n",
    "        y_pred_class1_total[idx] = pred\n",
    "\n",
    "    # accuracy = accuracy_score(y_test_class1_cat, y_pred_class1)\n",
    "    # print(f'📈 Accuracy: {accuracy:.4f}')\n",
    "    # precision = precision_score(y_test_class1_cat, y_pred_class1, average='macro', zero_division=0)\n",
    "    # print(f'📈 Precision: {precision:.4f}')\n",
    "    # recall = recall_score(y_test_class1_cat, y_pred_class1, average='macro')\n",
    "    # print(f'📈 Recall: {recall:.4f}')\n",
    "    # f1 = f1_score(y_test_class1_cat, y_pred_class1, average='macro')\n",
    "    # print(f'📈 F1: {f1:.4f}')  \n",
    "    # print(\"*\" * 50 + \"\\n\")\n",
    "    \n",
    "    predicciones_df.loc[indices_anomalia_test, 'Predicciones_Class1'] = y_pred_class1_total \n",
    "        \n",
    "normal_indices = predicciones_df[predicciones_df['Predicciones_Class2'] == \"RDOS\"].index\n",
    "predicciones_df.loc[normal_indices, 'Predicciones_Class1'] = \"RDOS\"\n",
    "normal_indices = predicciones_df[predicciones_df['Predicciones_Class2'] == \"Exfiltration\"].index\n",
    "predicciones_df.loc[normal_indices, 'Predicciones_Class1'] = \"Exfiltration\"\n",
    "normal_indices = predicciones_df[predicciones_df['Predicciones_Class2'] == \"C&C\"].index\n",
    "predicciones_df.loc[normal_indices, 'Predicciones_Class1'] = \"C&C\"\n",
    "normal_indices = predicciones_df[predicciones_df['Predicciones_Class2'] == \"crypto-ransomware\"].index\n",
    "predicciones_df.loc[normal_indices, 'Predicciones_Class1'] = \"crypto-ransomware\"\n",
    "normal_indices = predicciones_df[predicciones_df['Predicciones_Class2'] == \"Normal\"].index\n",
    "predicciones_df.loc[normal_indices, 'Predicciones_Class1'] = \"Normal\"\n",
    "os.makedirs(ruta_directorio, exist_ok=True)\n",
    "predicciones_df.to_csv(os.path.join(ruta_directorio, 'arq3.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6649121,
     "sourceId": 10725550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
